[{"categories":["python"],"content":"추상 클래스는 하나 이상의 추상 메소드를 포함하는 클래스입니다. Python에서는 어떻게 추상 클래스를 생성할까요?","date":"2024-07-13","objectID":"/posts/python-abstract-class/","series":null,"tags":["AbstractClass","package","ABC"],"title":"[Python] 추상 클래스","uri":"/posts/python-abstract-class/"},{"categories":["python"],"content":" 추상 메소드추상 메소드(abstract method)는 선언만 존재하고 구현은 존재하지 않는 메소드를 말합니다. 이는 하위 클래스에서 반드시 구현해야합니다. 추상 메소드는 하위 클래스에서 반드시 구현해야 하므로 해당 메소드가 강제적으로 구현됨을 보장할 수 있습니다. 또한 여러 클래스가 같은 이름의 메소드를 통해 동작하는 다형성을 제공하고 유연한 시스템을 제공합니다. ","date":"2024-07-13","objectID":"/posts/python-abstract-class/:1:0","series":null,"tags":["AbstractClass","package","ABC"],"title":"[Python] 추상 클래스","uri":"/posts/python-abstract-class/#추상-메소드"},{"categories":["python"],"content":" 추상 클래스추상 클래스(abstract class)는 하나 이상의 추상 메소드를 포함하는 클래스를 말합니다. 추상 클래스는 일반 클래스와 다르게 직접적으로 객체를 생성할 수 없고 반드시 상속받아서 하위 클래스에서 추상 메소드를 구현해야만 객체를 생성할 수 있습니다. ","date":"2024-07-13","objectID":"/posts/python-abstract-class/:2:0","series":null,"tags":["AbstractClass","package","ABC"],"title":"[Python] 추상 클래스","uri":"/posts/python-abstract-class/#추상-클래스"},{"categories":["python"],"content":" Python의 추상 클래스Python은 ABC(Abstract Base Class) 모듈을 통하여 추상 클래스 기능을 제공합니다. ABC 모듈은 클래스의 메타 클래스를 ABCMeta로 설정하여 추상 클래스를 선언할 수 있습니다. python from abc import ABC class AbstractClass(ABC): pass 혹은 메타 클래스 키워드를 직접 전달하며 추상 클래스를 선언할 수도 있습니다. python from abc import ABCMeta class AbstractClass(metaclass=ABCMeta): pass ","date":"2024-07-13","objectID":"/posts/python-abstract-class/:3:0","series":null,"tags":["AbstractClass","package","ABC"],"title":"[Python] 추상 클래스","uri":"/posts/python-abstract-class/#python의-추상-클래스"},{"categories":["python"],"content":" 추상 메서드 선언ABC 모듈의 abstractmethod 프러퍼티를 통해 추상 메서드를 선언할 수 있습니다. python from abc import ABC, abstractmethod class AbstractAnimal(ABC): @abstractmethod def move(self): pass class Dog(AbstractAnimal): def move(self): print(\"Move Dog!\") dog = Dog() dog.move() 다음과 같이 classmethod와 staticmethod 모두 마찬가지로 프러퍼티를 중첩하여 선언할 수 있습니다. python from abc import ABC, abstractmethod class AbstractAnimal(ABC): @classmethod @abstractmethod def abstract_class_method(cls): pass @staticmethod @abstractmethod def abstract_static_method(): pass class Dog(AbstractAnimal): @classmethod def abstract_class_method(cls): pass @staticmethod def abstract_static_method(): pass Info docstring을 작성한다면 pass를 작성하지 않아도 동작이 없음(no-op)을 표현할 수 있습니다. ","date":"2024-07-13","objectID":"/posts/python-abstract-class/:3:1","series":null,"tags":["AbstractClass","package","ABC"],"title":"[Python] 추상 클래스","uri":"/posts/python-abstract-class/#추상-메서드-선언"},{"categories":["python"],"content":" 추상 프로퍼티ABC 모듈의 abstractmethod와 property 데코레이터를 중첩하는 방식을 통해 추상 프러퍼티를 선언할 수 있습니다. 이때, property 데코레이터가 abstractmethod 위에 존재해야 합니다. python from abc import ABC, abstractmethod class AbstractAnimal(ABC): @property @abstractmethod def name(self): pass class Dog(AbstractAnimal): @property def name(self): return \"Dog\" ","date":"2024-07-13","objectID":"/posts/python-abstract-class/:3:2","series":null,"tags":["AbstractClass","package","ABC"],"title":"[Python] 추상 클래스","uri":"/posts/python-abstract-class/#추상-프로퍼티"},{"categories":["python"],"content":" 추상 변수만약 프러퍼티가 아닌 변수 형태로 선언해야 한다면 ABC 모듈에서 직접적으로 제공하는 방법은 없지만 몇 가지 방법이 존재합니다. 다음은 클래스에서 타입 어노테이션을 통해 변수를 초기화하지 않고 선언하는 것을 사용하는 방법입니다. python from abc import ABC, abstractmethod class AbstractAnimal(ABC): name: str @abstractmethod def move(self): pass class Dog(AbstractAnimal): def move(self): pass 하지만 해당 방법은 인스턴스 선언 시에 하위 클래스의 클래스 변수가 선언되어 있지 않아도 이를 파악할 수 없고 사용 시점에만 파악할 수 있다는 문제가 있습니다. python from abc import ABC, abstractmethod class AbstractAnimal(ABC): name: str @abstractmethod def move(self): pass class Dog(AbstractAnimal): def move(self): pass dog = Dog() dog.name # AttributeError: 'Dog' object has no attribute 'name' 위의 코드를 보면 dog 객체의 생성 시점에는 에러가 발생하지 않고 name을 호출하는 시점에 에러가 발생하는 것을 확인할 수 있습니다. 다른 방법은 추상 프러퍼티로 선언한 후 클래스 변수로 대체하는 방법이 존재합니다. python from abc import ABC, abstractmethod class AbstractAnimal(ABC): @property @abstractmethod def name(self): pass class Dog(AbstractAnimal): name = \"Dog\" ","date":"2024-07-13","objectID":"/posts/python-abstract-class/:3:3","series":null,"tags":["AbstractClass","package","ABC"],"title":"[Python] 추상 클래스","uri":"/posts/python-abstract-class/#추상-변수"},{"categories":["python"],"content":" 추상 클래스 객체화추상 클래스는 직접적으로 객체화하는 것이 불가능합니다. python from abc import ABC, abstractmethod class AbstractAnimal(ABC): @abstractmethod def move(self): pass animal = AbstractAnimal() # TypeError: Can't instantiate abstract class AbstractAnimal with abstract method move ","date":"2024-07-13","objectID":"/posts/python-abstract-class/:3:4","series":null,"tags":["AbstractClass","package","ABC"],"title":"[Python] 추상 클래스","uri":"/posts/python-abstract-class/#추상-클래스-객체화"},{"categories":["DevOps"],"content":"컨테이너는 호스트 운영 체제에서 응용 프로그램과 그 종속성을 포함하여 일관된 환경에서 실행될 수 있도록 격리된 경량 가상화 기술입니다.","date":"2024-07-11","objectID":"/posts/k8s-container/","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/"},{"categories":["DevOps"],"content":" 가상화가상화는 물리적 하드웨어 리소스를 추상화하여 논리적인 리소스로 변환하여 물리적 하드웨어를 보다 효율적으로 활용할 수 있도록 하는 기술입니다. 즉, 하나의 컴퓨터를 마치 여러 대의 컴퓨터처럼 나누어 사용하는 방법입니다. 이는 하나의 컴퓨터에서 하나의 서버를 실행하는 것이 아닌 자원 사용량에 따라 하나의 컴퓨터에서 여러 서버를 실행할 수 있도록 합니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:1:0","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#가상화"},{"categories":["DevOps"],"content":" 가상화 장점가상화는 다음과 같은 장점을 제공합니다. 효율적인 리소스 사용: 가상화를 통해 하나의 컴퓨터에서 하나의 서버를 실행하는 것이 아니라 필요에 따라 서버를 사용하고 반환하는 방식으로 활용하며 더 효율적으로 리소스를 사용할 수 있습니다. 자동화된 IT 관리: 물리적 리소스를 가상화하여 소프트웨어 도구로 관리할 수 있게 됩니다. 이는 가상 머신에 대한 템플릿을 정의하고 일관된 방식으로 인프라를 복제할 수 있게 합니다. 신속한 재해 복구: 물리적 리소스에 문제가 발생하는 경우 물리적 서버를 수리하는데 많은 시간이 소요됩니다. 하지만 가상화된 환경에서는 다른 리소스를 통해 즉각적인 복원이 가능합니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:1:1","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#가상화-장점"},{"categories":["DevOps"],"content":" Virtual Machine (VM) 가상 머신(VM)은 실행 중인 어플리케이션과 운영 체제를 포함하여 컴퓨터와 거의 동일한 모든 기능을 수행할 수 있는 컴퓨터의 가상화된 인스턴스입니다. 가상 머신은 하이퍼바이저를 통해 컴퓨팅 리소스를 할당받습니다. 하이퍼바이저는 물리적 머신의 리소스를 추상화하여 가상 머신에 할당하는 소프트웨어입니다. 각각의 가상 머신은 자신만의 OS(Guest OS)를 가지기 때문에 다른 가상 머신과 완전히 격리되어있으며 개별의 커널을 가지게 됩니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:2:0","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#virtual-machine-vm"},{"categories":["DevOps"],"content":" 컨테이너 컨테이너는 호스트 운영 체제에서 어플리케이션을 실행하기 위해 격리된 경량 인스턴스입니다. 컨테이너는 기존 호스트 OS의 커널을 공유하며 유저 모드의 어플리케이션을 격리하여 사용자가 개별 어플리케이션을 사용하고 있는 것처럼 제공합니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:3:0","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#컨테이너"},{"categories":["DevOps"],"content":" 컨테이너 vs VM Info 컨테이너와 VM은 다음과 같이 정의할 수 있습니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:3:1","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#컨테이너-vs-vm"},{"categories":["DevOps"],"content":" OS(VM) = kernel + filesystem/libraries Image(Container) = filesystem/libraries 컨테이너와 가상 머신은 각각 장단점을 가지고 있고 필요한 상황에 따라 선택하여 사용하거나 혼합하여 사용하기도 합니다. AWS, GCP 등의 CSP에서는 가상 머신 기반으로 인프라를 구성하고 컨테이너 형태로 어플리케이션을 구성하여 제공하고 있습니다. 무거운 리소스와 부팅 시간가상 머신은 개별 커널을 보유하기 때문에 메모리와 디스크 사용량이 많고 개별 커널을 부팅해야하므로 부팅 시간또한 매우 깁니다. 반면 컨테이너는 호스트 OS의 커널을 공유하므로 메모리와 디스크 샤용량이 적고 빠르게 시작될 수 있다는 장점이 있습니다. 컨테이너는 경량화되어있어 좋은 이식성을 가지고 있어 도커 및 쿠버네티스 등에서 다양하게 활용되고 있습니다. 호스트 OS 의존성가상 머신은 개별 커널 위에서 동작하기 때문에 다양한 운영 체제를 동일한 호스트에서 수행할 수 있습니다. 예를 들어, Windows 서버 위에서 Linux 가상 머신을 실행시킬 수 있습니다. 반면 컨테이너는 호스트 OS의 커널에 의존하므로 다양한 운영 체제를 활용할 수는 없습니다. 보안 및 격리가상 머신은 하드웨어 수준에서 완전히 격리되어 보안 및 안정성이 매우 높습니다. 하나의 가상 머신에서 발생한 문제가 다른 가상 머신에 영향을 끼치지 않습니다. 반면 컨테이너는 고립된 파일 시스템, 네트워크, 프로세스 공간 등을 가지긴 하지만 비교적 VM에 비하여 낮은 격리 수준을 갖습니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:0:0","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#heading"},{"categories":["DevOps"],"content":" OS(VM) = kernel + filesystem/libraries Image(Container) = filesystem/libraries 컨테이너와 가상 머신은 각각 장단점을 가지고 있고 필요한 상황에 따라 선택하여 사용하거나 혼합하여 사용하기도 합니다. AWS, GCP 등의 CSP에서는 가상 머신 기반으로 인프라를 구성하고 컨테이너 형태로 어플리케이션을 구성하여 제공하고 있습니다. 무거운 리소스와 부팅 시간가상 머신은 개별 커널을 보유하기 때문에 메모리와 디스크 사용량이 많고 개별 커널을 부팅해야하므로 부팅 시간또한 매우 깁니다. 반면 컨테이너는 호스트 OS의 커널을 공유하므로 메모리와 디스크 샤용량이 적고 빠르게 시작될 수 있다는 장점이 있습니다. 컨테이너는 경량화되어있어 좋은 이식성을 가지고 있어 도커 및 쿠버네티스 등에서 다양하게 활용되고 있습니다. 호스트 OS 의존성가상 머신은 개별 커널 위에서 동작하기 때문에 다양한 운영 체제를 동일한 호스트에서 수행할 수 있습니다. 예를 들어, Windows 서버 위에서 Linux 가상 머신을 실행시킬 수 있습니다. 반면 컨테이너는 호스트 OS의 커널에 의존하므로 다양한 운영 체제를 활용할 수는 없습니다. 보안 및 격리가상 머신은 하드웨어 수준에서 완전히 격리되어 보안 및 안정성이 매우 높습니다. 하나의 가상 머신에서 발생한 문제가 다른 가상 머신에 영향을 끼치지 않습니다. 반면 컨테이너는 고립된 파일 시스템, 네트워크, 프로세스 공간 등을 가지긴 하지만 비교적 VM에 비하여 낮은 격리 수준을 갖습니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:0:0","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#무거운-리소스와-부팅-시간"},{"categories":["DevOps"],"content":" OS(VM) = kernel + filesystem/libraries Image(Container) = filesystem/libraries 컨테이너와 가상 머신은 각각 장단점을 가지고 있고 필요한 상황에 따라 선택하여 사용하거나 혼합하여 사용하기도 합니다. AWS, GCP 등의 CSP에서는 가상 머신 기반으로 인프라를 구성하고 컨테이너 형태로 어플리케이션을 구성하여 제공하고 있습니다. 무거운 리소스와 부팅 시간가상 머신은 개별 커널을 보유하기 때문에 메모리와 디스크 사용량이 많고 개별 커널을 부팅해야하므로 부팅 시간또한 매우 깁니다. 반면 컨테이너는 호스트 OS의 커널을 공유하므로 메모리와 디스크 샤용량이 적고 빠르게 시작될 수 있다는 장점이 있습니다. 컨테이너는 경량화되어있어 좋은 이식성을 가지고 있어 도커 및 쿠버네티스 등에서 다양하게 활용되고 있습니다. 호스트 OS 의존성가상 머신은 개별 커널 위에서 동작하기 때문에 다양한 운영 체제를 동일한 호스트에서 수행할 수 있습니다. 예를 들어, Windows 서버 위에서 Linux 가상 머신을 실행시킬 수 있습니다. 반면 컨테이너는 호스트 OS의 커널에 의존하므로 다양한 운영 체제를 활용할 수는 없습니다. 보안 및 격리가상 머신은 하드웨어 수준에서 완전히 격리되어 보안 및 안정성이 매우 높습니다. 하나의 가상 머신에서 발생한 문제가 다른 가상 머신에 영향을 끼치지 않습니다. 반면 컨테이너는 고립된 파일 시스템, 네트워크, 프로세스 공간 등을 가지긴 하지만 비교적 VM에 비하여 낮은 격리 수준을 갖습니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:0:0","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#호스트-os-의존성"},{"categories":["DevOps"],"content":" OS(VM) = kernel + filesystem/libraries Image(Container) = filesystem/libraries 컨테이너와 가상 머신은 각각 장단점을 가지고 있고 필요한 상황에 따라 선택하여 사용하거나 혼합하여 사용하기도 합니다. AWS, GCP 등의 CSP에서는 가상 머신 기반으로 인프라를 구성하고 컨테이너 형태로 어플리케이션을 구성하여 제공하고 있습니다. 무거운 리소스와 부팅 시간가상 머신은 개별 커널을 보유하기 때문에 메모리와 디스크 사용량이 많고 개별 커널을 부팅해야하므로 부팅 시간또한 매우 깁니다. 반면 컨테이너는 호스트 OS의 커널을 공유하므로 메모리와 디스크 샤용량이 적고 빠르게 시작될 수 있다는 장점이 있습니다. 컨테이너는 경량화되어있어 좋은 이식성을 가지고 있어 도커 및 쿠버네티스 등에서 다양하게 활용되고 있습니다. 호스트 OS 의존성가상 머신은 개별 커널 위에서 동작하기 때문에 다양한 운영 체제를 동일한 호스트에서 수행할 수 있습니다. 예를 들어, Windows 서버 위에서 Linux 가상 머신을 실행시킬 수 있습니다. 반면 컨테이너는 호스트 OS의 커널에 의존하므로 다양한 운영 체제를 활용할 수는 없습니다. 보안 및 격리가상 머신은 하드웨어 수준에서 완전히 격리되어 보안 및 안정성이 매우 높습니다. 하나의 가상 머신에서 발생한 문제가 다른 가상 머신에 영향을 끼치지 않습니다. 반면 컨테이너는 고립된 파일 시스템, 네트워크, 프로세스 공간 등을 가지긴 하지만 비교적 VM에 비하여 낮은 격리 수준을 갖습니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:0:0","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#보안-및-격리"},{"categories":["DevOps"],"content":" 컨테이너 표준컨테이너 기술이 발전함에 따라, 다양한 표준과 규격이 도입되었습니다. 이러한 표준은 컨테이너 이미지의 형식과 실행 환경을 정의하고, 상호 운용성을 보장하며, 컨테이너 생태계를 더욱 견고하고 통합된 환경으로 발전시키는 데 기여하고 있습니다. OCI(Open Container Initiative)는 이러한 컨테이너의 표준입니다. 도커는 이러한 OCI 표준을 기반으로 containerd를 개발하여 도커 엔진에 탑재하고 있습니다. Red Hat, Intel, IBM에서는 OCI 표준을 준수하는 CRI-O를 제안하였습니다. 이렇게 다른 컨테이너 실행 환경인 containerd와 CRI-O 모두 OCI 표준을 준수하기에 쿠버네티스 등에서는 해당 런타임으로 생성된 컨테이너 이미지를 모두 지원하고 있습니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:0:1","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#컨테이너-표준"},{"categories":["DevOps"],"content":" 컨테이너의 구조컨테이너는 chroot라는 리눅스 시스템 콜로 시작하게 되었습니다. chroot는 프로세스 루트 디렉토리를 변경하는 리눅스 시스템 콜로 ssh 등으로 접속한 유저가 특정 디렉토리에만 접근 가능하도록 제어하는 용도로 설계되었습니다. chroot는 Change Root Directory의 약어로 특정 프로세스의 루트 디렉토리를 설정하면 사용자가 해당 루트 디렉토리 밖으로 나가지 못하도록 가두는 명령어입니다. chroot로 갇힌 프로세스는 해당 디렉토리 밖의 라이브러리 등을 사용하지 못하므로 기본적인 동작을 할 수 없었고, 필요한 커맨드 프로그램과 라이브러리 등을 경로에 함께 추가해야 합니다. 하지만 매번 모든 라이브러리와 프로그램 등을 직접 복사해 넣는 것은 복잡한 과정이었고 이를 해결하기 위해 프로그램 실행에 필요한 라이브러리와 프로그램을 미리 모아둔 이미지를 사용하게 됩니다. 하지만 chroot는 탈옥이 가능하고 격리되지 않으며 루트 권한 및 리소스 무제한 문제 등 다양한 문제를 가지고 있었고 컨테이너는 chroot를 사용하지 않고 linux의 기본 명령어인 pivot_root, namespace, cgroups로 컨테이너를 생성하게 됩니다. 즉, 컨테이너는 리눅스 명령어로 생성된 개별 프로세스 입니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:1:0","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#컨테이너의-구조"},{"categories":["DevOps"],"content":" pivot rootpivot_root는 리눅스 시스템에서 루트 파일 시스템을 변경하는 명령어입니다. chroot의 경우 루트 파일 시스템이 동일하기 때문에 탈옥이 가능하다는 문제가 있습니다. pivot_root를 통해 루트 파일 시스템을 변경하여 탈옥을 막을 수 있습니다. 이때, 변경되는 파일 시스템이 호스트에 영향을 주지 않기 위해서 마운트 네임스페이스를 분리하게 됩니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:1:1","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#pivot-root"},{"categories":["DevOps"],"content":" 네임스페이스리눅스 커널의 네임스페이스는 프로세스에 격리된 환경과 리소스를 제공합니다. 네임스페이스는 다음과 같은 특징이 있습니다. 네임스페이스 안에서의 변경은 내부의 프로세스에만 보이고 외부 프로세스에는 보이지 않습니다. 모든 프로세스는 네임스페이스 타입 별로 특정 네임스페이스에 속합니다. 자식 프로세스는 부모 프로세스의 네임스페이스를 상속받습니다. 네임스페이스는 격리하는 자원에 따라 다음과 같이 구분됩니다. 마운트 네임스페이스마운트 네임스페이스는 프로세스와 그 자식 프로세스에게 다른 파일 시스템 마운트 포인트를 제공합니다. 네임스페이스의 프로세스들에게 보여지는 마운트 포인트를 격리시키고 격리된 마운트 포인트들은 각 프로세스에게 단일 디렉토리 구조로 보이게 됩니다. 이때 마운트 네임스페이스의 종류는 다음과 같습니다. private: 각 마운트 포인트가 다른 마운트 포인트에 반영되지 않는 방법 shared: 각 마운트 포인트가 다른 마운트 포인트에 반영되어 보여지는 방법 slave: 파일시스템 하위에서 새로운 마운트는 파일시스템에 반영되나, 반대는 반영되지 않는 방법 bash # 마운트할 디렉터리를 생성 $ mkdir /tmp/mount_ns # 현재 bash 프로세스를 마운트 네임스페이스로 이동 # unshare은 네임스페이스 생성 명령어이고 -m이 마운트 네임스페이스를 생성하도록 지정 $ sudo unshare -m /bin/bash # 현재 네임스페이스의 관련 inode 번호 확인 $ readlink /proc/$$/ns/mnt mnt:[4026532226] # tmpfs이라는 임시 파일 시스템 타입으로 마운트 $ mount -t tmpfs tmpfs /tmp/mount_ns # 임시 파일 생성 $ touch /tmp/mount_ns/a $ ls /tmp/mount_ns/ a # 새로 생성한 마운트 지점 확인 $ mount | grep mount_ns tmpfs on /tmp/mount_ns type tmpfs (rw,relatime,seclabel) 새로운 터미널에서 마운트 지점을 확인하면 기존 터미널이 마운트 네임스페이스로 격리되었기 때문에 새로운 터미널에서 보이지 않는 것을 확인할 수 있습니다. bash # 새로운 터미널 # 다른 네임스페이스이기에 네임스페이스의 inode 번호가 다른 것을 확인할 수 있다. $ readlink /proc/$$/ns/mnt mnt:[4026531841] # ls를 사용하더라도 디렉토리는 보이지만 격리된 a 파일은 보이지 않는다. $ ls /tmp/mount_ns/ # 마운트 지점 확인 또한 되지 않는다. $ mount | grep mount_ns UTS 네임스페이스UTS 네임스페이스는 호스트 명을 서버와 다르게 사용할 수 있도록 합니다. UTS는 Unix Time Sharing인 시분할의 약자로 여러 사용자의 요청을 CPU 시간을 쪼개서 사용하며 여러 사용자의 환경 별로 호스트 명이나 도메인 명을 구분할 수 있도록 격리를 제공합니다. bash $ hostname ubuntu1804 $ readlink /proc/$$/ns/uts uts:[4026531838] # 현재 bash 프로세스에서 uts 네임스페이스를 격리 $ sudo unshare --uts /bin/bash # 호스트 명 변경 $ hostname MyHost $ hostname MyHost $ readlink /proc/$$/ns/uts uts:[4026532227] 새로운 터미널에서는 호스트 명이 유지되는 것을 확인할 수 있습니다. bash $ readlink /proc/$$/ns/uts uts:[4026531838] $ hostname ubuntu1804 IPC 네임스페이스IPC 네임스페이스는 Inter Process Communication을 지원하는 공유 메모리 및 메세지 큐 등의 리소스에 대한 격리를 제공합니다. bash $ sudo unshare --ipc /bin/bash # 공유 메모리 생성 $ ipcmk -M 2000 Shared memory id: 0 $ ipcs -m ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0x809712ea 0 root 644 2000 0 $ readlink /proc/$$/ns/ipc ipc:[4026531839] 새로운 터미널에서 공유 메모리를 확인하면 다음과 같습니다. bash # 공유 메모리 생성 $ ipcmk -M 1000 Shared memory id: 0 $ ipcs -m ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0xdf4a6453 0 root 644 1000 0 $ readlink /proc/$$/ns/ipc ipc:[4026532228] PID 네임스페이스pid는 프로세스 별로 부여되는 고유 번호로 프로세스 트리 최상위는 init 프로세스라는 pid 1을 갖는 프로세스가 존재합니다. PID 네임스페이스는 pid 리소스를 격리합니다. PID 네임스페이스를 격리하면 격리된 컨테이너 안에도 pid 1로 시작하는 트리 구조를 가지게 됩니다. PID 네임스페이스는 부모 네임 스페이스와 자식 네임스페이스가 중첩되는 구조로 자식 네임스페이스의 모든 프로세스들은 부모 네임스페이스의 pid와 해당 네임스페이스의 pid를 모두 가지게 됩니다. bash $ echo $$ 646748 $ sudo unshare --pid --fork --mount-proc /bin/bash $ echo $$ 1 // 현재 실행중인 프로세스가 컨테이너의 프로세스만 보인다. $ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.5 233096 5112 pts/1 S 14:24 0:00 /bin/bash root 20 0.0 0.2 232520 2780 pts/1 R+ 14:25 0:00 ps aux cgroup 네임스페이스cgroup은 프로세스에 할당할 시스템 리소스(CPU, Memory, Network)에 대한 제어를 제공합니다. cgroup은 파일시스템을 기반으로 파일시스템에 디렉토리를 만들고 파일을 수정하는 방식으로 시스템 리소스를 관리합니다. 이 때무에 컨테이너에서 cgroup이 관리하는 파일시스템에 접근하게 되면 호스트의 시스템 리소스를 건드릴 수 있습니다. cgroup 네임스페이스는 cgroup 파일시스템(/sys/fs/cgroup)을 격리합니다. 네트워크 네임스페이스네트워크 네임스페이스는 컨테이너의 네트워크를 가상화하여 네트워크 상 별도의 노드로 취급하도록 합니다. bash $ sudo unshare --net /bin/bash $ lsns -t net -p $$ NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026532226 net 2 655964 root unassigned /bin/bash $ lsns -t net -p 1 NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026531840 net 113 1 root unassigned /usr/lib/systemd/systemd --switc NS는 네트워크 네임스페이스의 inode값으로 현재 프로세스와 init 프로세스의 inode 값이 서로 다름을 확인할 수 있습니다. User 네임스페이스User 네임스페이스는 유저가 컨테이너 안에서만 루트 유저로 동작할 수 있도록 합니다. 각 User 네임스페이스는 독립적인 사용자 및 그룹 ID를 할당하여 호스트 시스템의 사용자와 격리된 환경을 제공합니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:1:2","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#네임스페이스"},{"categories":["DevOps"],"content":" 네임스페이스리눅스 커널의 네임스페이스는 프로세스에 격리된 환경과 리소스를 제공합니다. 네임스페이스는 다음과 같은 특징이 있습니다. 네임스페이스 안에서의 변경은 내부의 프로세스에만 보이고 외부 프로세스에는 보이지 않습니다. 모든 프로세스는 네임스페이스 타입 별로 특정 네임스페이스에 속합니다. 자식 프로세스는 부모 프로세스의 네임스페이스를 상속받습니다. 네임스페이스는 격리하는 자원에 따라 다음과 같이 구분됩니다. 마운트 네임스페이스마운트 네임스페이스는 프로세스와 그 자식 프로세스에게 다른 파일 시스템 마운트 포인트를 제공합니다. 네임스페이스의 프로세스들에게 보여지는 마운트 포인트를 격리시키고 격리된 마운트 포인트들은 각 프로세스에게 단일 디렉토리 구조로 보이게 됩니다. 이때 마운트 네임스페이스의 종류는 다음과 같습니다. private: 각 마운트 포인트가 다른 마운트 포인트에 반영되지 않는 방법 shared: 각 마운트 포인트가 다른 마운트 포인트에 반영되어 보여지는 방법 slave: 파일시스템 하위에서 새로운 마운트는 파일시스템에 반영되나, 반대는 반영되지 않는 방법 bash # 마운트할 디렉터리를 생성 $ mkdir /tmp/mount_ns # 현재 bash 프로세스를 마운트 네임스페이스로 이동 # unshare은 네임스페이스 생성 명령어이고 -m이 마운트 네임스페이스를 생성하도록 지정 $ sudo unshare -m /bin/bash # 현재 네임스페이스의 관련 inode 번호 확인 $ readlink /proc/$$/ns/mnt mnt:[4026532226] # tmpfs이라는 임시 파일 시스템 타입으로 마운트 $ mount -t tmpfs tmpfs /tmp/mount_ns # 임시 파일 생성 $ touch /tmp/mount_ns/a $ ls /tmp/mount_ns/ a # 새로 생성한 마운트 지점 확인 $ mount | grep mount_ns tmpfs on /tmp/mount_ns type tmpfs (rw,relatime,seclabel) 새로운 터미널에서 마운트 지점을 확인하면 기존 터미널이 마운트 네임스페이스로 격리되었기 때문에 새로운 터미널에서 보이지 않는 것을 확인할 수 있습니다. bash # 새로운 터미널 # 다른 네임스페이스이기에 네임스페이스의 inode 번호가 다른 것을 확인할 수 있다. $ readlink /proc/$$/ns/mnt mnt:[4026531841] # ls를 사용하더라도 디렉토리는 보이지만 격리된 a 파일은 보이지 않는다. $ ls /tmp/mount_ns/ # 마운트 지점 확인 또한 되지 않는다. $ mount | grep mount_ns UTS 네임스페이스UTS 네임스페이스는 호스트 명을 서버와 다르게 사용할 수 있도록 합니다. UTS는 Unix Time Sharing인 시분할의 약자로 여러 사용자의 요청을 CPU 시간을 쪼개서 사용하며 여러 사용자의 환경 별로 호스트 명이나 도메인 명을 구분할 수 있도록 격리를 제공합니다. bash $ hostname ubuntu1804 $ readlink /proc/$$/ns/uts uts:[4026531838] # 현재 bash 프로세스에서 uts 네임스페이스를 격리 $ sudo unshare --uts /bin/bash # 호스트 명 변경 $ hostname MyHost $ hostname MyHost $ readlink /proc/$$/ns/uts uts:[4026532227] 새로운 터미널에서는 호스트 명이 유지되는 것을 확인할 수 있습니다. bash $ readlink /proc/$$/ns/uts uts:[4026531838] $ hostname ubuntu1804 IPC 네임스페이스IPC 네임스페이스는 Inter Process Communication을 지원하는 공유 메모리 및 메세지 큐 등의 리소스에 대한 격리를 제공합니다. bash $ sudo unshare --ipc /bin/bash # 공유 메모리 생성 $ ipcmk -M 2000 Shared memory id: 0 $ ipcs -m ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0x809712ea 0 root 644 2000 0 $ readlink /proc/$$/ns/ipc ipc:[4026531839] 새로운 터미널에서 공유 메모리를 확인하면 다음과 같습니다. bash # 공유 메모리 생성 $ ipcmk -M 1000 Shared memory id: 0 $ ipcs -m ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0xdf4a6453 0 root 644 1000 0 $ readlink /proc/$$/ns/ipc ipc:[4026532228] PID 네임스페이스pid는 프로세스 별로 부여되는 고유 번호로 프로세스 트리 최상위는 init 프로세스라는 pid 1을 갖는 프로세스가 존재합니다. PID 네임스페이스는 pid 리소스를 격리합니다. PID 네임스페이스를 격리하면 격리된 컨테이너 안에도 pid 1로 시작하는 트리 구조를 가지게 됩니다. PID 네임스페이스는 부모 네임 스페이스와 자식 네임스페이스가 중첩되는 구조로 자식 네임스페이스의 모든 프로세스들은 부모 네임스페이스의 pid와 해당 네임스페이스의 pid를 모두 가지게 됩니다. bash $ echo $$ 646748 $ sudo unshare --pid --fork --mount-proc /bin/bash $ echo $$ 1 // 현재 실행중인 프로세스가 컨테이너의 프로세스만 보인다. $ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.5 233096 5112 pts/1 S 14:24 0:00 /bin/bash root 20 0.0 0.2 232520 2780 pts/1 R+ 14:25 0:00 ps aux cgroup 네임스페이스cgroup은 프로세스에 할당할 시스템 리소스(CPU, Memory, Network)에 대한 제어를 제공합니다. cgroup은 파일시스템을 기반으로 파일시스템에 디렉토리를 만들고 파일을 수정하는 방식으로 시스템 리소스를 관리합니다. 이 때무에 컨테이너에서 cgroup이 관리하는 파일시스템에 접근하게 되면 호스트의 시스템 리소스를 건드릴 수 있습니다. cgroup 네임스페이스는 cgroup 파일시스템(/sys/fs/cgroup)을 격리합니다. 네트워크 네임스페이스네트워크 네임스페이스는 컨테이너의 네트워크를 가상화하여 네트워크 상 별도의 노드로 취급하도록 합니다. bash $ sudo unshare --net /bin/bash $ lsns -t net -p $$ NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026532226 net 2 655964 root unassigned /bin/bash $ lsns -t net -p 1 NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026531840 net 113 1 root unassigned /usr/lib/systemd/systemd --switc NS는 네트워크 네임스페이스의 inode값으로 현재 프로세스와 init 프로세스의 inode 값이 서로 다름을 확인할 수 있습니다. User 네임스페이스User 네임스페이스는 유저가 컨테이너 안에서만 루트 유저로 동작할 수 있도록 합니다. 각 User 네임스페이스는 독립적인 사용자 및 그룹 ID를 할당하여 호스트 시스템의 사용자와 격리된 환경을 제공합니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:1:2","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#마운트-네임스페이스"},{"categories":["DevOps"],"content":" 네임스페이스리눅스 커널의 네임스페이스는 프로세스에 격리된 환경과 리소스를 제공합니다. 네임스페이스는 다음과 같은 특징이 있습니다. 네임스페이스 안에서의 변경은 내부의 프로세스에만 보이고 외부 프로세스에는 보이지 않습니다. 모든 프로세스는 네임스페이스 타입 별로 특정 네임스페이스에 속합니다. 자식 프로세스는 부모 프로세스의 네임스페이스를 상속받습니다. 네임스페이스는 격리하는 자원에 따라 다음과 같이 구분됩니다. 마운트 네임스페이스마운트 네임스페이스는 프로세스와 그 자식 프로세스에게 다른 파일 시스템 마운트 포인트를 제공합니다. 네임스페이스의 프로세스들에게 보여지는 마운트 포인트를 격리시키고 격리된 마운트 포인트들은 각 프로세스에게 단일 디렉토리 구조로 보이게 됩니다. 이때 마운트 네임스페이스의 종류는 다음과 같습니다. private: 각 마운트 포인트가 다른 마운트 포인트에 반영되지 않는 방법 shared: 각 마운트 포인트가 다른 마운트 포인트에 반영되어 보여지는 방법 slave: 파일시스템 하위에서 새로운 마운트는 파일시스템에 반영되나, 반대는 반영되지 않는 방법 bash # 마운트할 디렉터리를 생성 $ mkdir /tmp/mount_ns # 현재 bash 프로세스를 마운트 네임스페이스로 이동 # unshare은 네임스페이스 생성 명령어이고 -m이 마운트 네임스페이스를 생성하도록 지정 $ sudo unshare -m /bin/bash # 현재 네임스페이스의 관련 inode 번호 확인 $ readlink /proc/$$/ns/mnt mnt:[4026532226] # tmpfs이라는 임시 파일 시스템 타입으로 마운트 $ mount -t tmpfs tmpfs /tmp/mount_ns # 임시 파일 생성 $ touch /tmp/mount_ns/a $ ls /tmp/mount_ns/ a # 새로 생성한 마운트 지점 확인 $ mount | grep mount_ns tmpfs on /tmp/mount_ns type tmpfs (rw,relatime,seclabel) 새로운 터미널에서 마운트 지점을 확인하면 기존 터미널이 마운트 네임스페이스로 격리되었기 때문에 새로운 터미널에서 보이지 않는 것을 확인할 수 있습니다. bash # 새로운 터미널 # 다른 네임스페이스이기에 네임스페이스의 inode 번호가 다른 것을 확인할 수 있다. $ readlink /proc/$$/ns/mnt mnt:[4026531841] # ls를 사용하더라도 디렉토리는 보이지만 격리된 a 파일은 보이지 않는다. $ ls /tmp/mount_ns/ # 마운트 지점 확인 또한 되지 않는다. $ mount | grep mount_ns UTS 네임스페이스UTS 네임스페이스는 호스트 명을 서버와 다르게 사용할 수 있도록 합니다. UTS는 Unix Time Sharing인 시분할의 약자로 여러 사용자의 요청을 CPU 시간을 쪼개서 사용하며 여러 사용자의 환경 별로 호스트 명이나 도메인 명을 구분할 수 있도록 격리를 제공합니다. bash $ hostname ubuntu1804 $ readlink /proc/$$/ns/uts uts:[4026531838] # 현재 bash 프로세스에서 uts 네임스페이스를 격리 $ sudo unshare --uts /bin/bash # 호스트 명 변경 $ hostname MyHost $ hostname MyHost $ readlink /proc/$$/ns/uts uts:[4026532227] 새로운 터미널에서는 호스트 명이 유지되는 것을 확인할 수 있습니다. bash $ readlink /proc/$$/ns/uts uts:[4026531838] $ hostname ubuntu1804 IPC 네임스페이스IPC 네임스페이스는 Inter Process Communication을 지원하는 공유 메모리 및 메세지 큐 등의 리소스에 대한 격리를 제공합니다. bash $ sudo unshare --ipc /bin/bash # 공유 메모리 생성 $ ipcmk -M 2000 Shared memory id: 0 $ ipcs -m ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0x809712ea 0 root 644 2000 0 $ readlink /proc/$$/ns/ipc ipc:[4026531839] 새로운 터미널에서 공유 메모리를 확인하면 다음과 같습니다. bash # 공유 메모리 생성 $ ipcmk -M 1000 Shared memory id: 0 $ ipcs -m ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0xdf4a6453 0 root 644 1000 0 $ readlink /proc/$$/ns/ipc ipc:[4026532228] PID 네임스페이스pid는 프로세스 별로 부여되는 고유 번호로 프로세스 트리 최상위는 init 프로세스라는 pid 1을 갖는 프로세스가 존재합니다. PID 네임스페이스는 pid 리소스를 격리합니다. PID 네임스페이스를 격리하면 격리된 컨테이너 안에도 pid 1로 시작하는 트리 구조를 가지게 됩니다. PID 네임스페이스는 부모 네임 스페이스와 자식 네임스페이스가 중첩되는 구조로 자식 네임스페이스의 모든 프로세스들은 부모 네임스페이스의 pid와 해당 네임스페이스의 pid를 모두 가지게 됩니다. bash $ echo $$ 646748 $ sudo unshare --pid --fork --mount-proc /bin/bash $ echo $$ 1 // 현재 실행중인 프로세스가 컨테이너의 프로세스만 보인다. $ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.5 233096 5112 pts/1 S 14:24 0:00 /bin/bash root 20 0.0 0.2 232520 2780 pts/1 R+ 14:25 0:00 ps aux cgroup 네임스페이스cgroup은 프로세스에 할당할 시스템 리소스(CPU, Memory, Network)에 대한 제어를 제공합니다. cgroup은 파일시스템을 기반으로 파일시스템에 디렉토리를 만들고 파일을 수정하는 방식으로 시스템 리소스를 관리합니다. 이 때무에 컨테이너에서 cgroup이 관리하는 파일시스템에 접근하게 되면 호스트의 시스템 리소스를 건드릴 수 있습니다. cgroup 네임스페이스는 cgroup 파일시스템(/sys/fs/cgroup)을 격리합니다. 네트워크 네임스페이스네트워크 네임스페이스는 컨테이너의 네트워크를 가상화하여 네트워크 상 별도의 노드로 취급하도록 합니다. bash $ sudo unshare --net /bin/bash $ lsns -t net -p $$ NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026532226 net 2 655964 root unassigned /bin/bash $ lsns -t net -p 1 NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026531840 net 113 1 root unassigned /usr/lib/systemd/systemd --switc NS는 네트워크 네임스페이스의 inode값으로 현재 프로세스와 init 프로세스의 inode 값이 서로 다름을 확인할 수 있습니다. User 네임스페이스User 네임스페이스는 유저가 컨테이너 안에서만 루트 유저로 동작할 수 있도록 합니다. 각 User 네임스페이스는 독립적인 사용자 및 그룹 ID를 할당하여 호스트 시스템의 사용자와 격리된 환경을 제공합니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:1:2","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#uts-네임스페이스"},{"categories":["DevOps"],"content":" 네임스페이스리눅스 커널의 네임스페이스는 프로세스에 격리된 환경과 리소스를 제공합니다. 네임스페이스는 다음과 같은 특징이 있습니다. 네임스페이스 안에서의 변경은 내부의 프로세스에만 보이고 외부 프로세스에는 보이지 않습니다. 모든 프로세스는 네임스페이스 타입 별로 특정 네임스페이스에 속합니다. 자식 프로세스는 부모 프로세스의 네임스페이스를 상속받습니다. 네임스페이스는 격리하는 자원에 따라 다음과 같이 구분됩니다. 마운트 네임스페이스마운트 네임스페이스는 프로세스와 그 자식 프로세스에게 다른 파일 시스템 마운트 포인트를 제공합니다. 네임스페이스의 프로세스들에게 보여지는 마운트 포인트를 격리시키고 격리된 마운트 포인트들은 각 프로세스에게 단일 디렉토리 구조로 보이게 됩니다. 이때 마운트 네임스페이스의 종류는 다음과 같습니다. private: 각 마운트 포인트가 다른 마운트 포인트에 반영되지 않는 방법 shared: 각 마운트 포인트가 다른 마운트 포인트에 반영되어 보여지는 방법 slave: 파일시스템 하위에서 새로운 마운트는 파일시스템에 반영되나, 반대는 반영되지 않는 방법 bash # 마운트할 디렉터리를 생성 $ mkdir /tmp/mount_ns # 현재 bash 프로세스를 마운트 네임스페이스로 이동 # unshare은 네임스페이스 생성 명령어이고 -m이 마운트 네임스페이스를 생성하도록 지정 $ sudo unshare -m /bin/bash # 현재 네임스페이스의 관련 inode 번호 확인 $ readlink /proc/$$/ns/mnt mnt:[4026532226] # tmpfs이라는 임시 파일 시스템 타입으로 마운트 $ mount -t tmpfs tmpfs /tmp/mount_ns # 임시 파일 생성 $ touch /tmp/mount_ns/a $ ls /tmp/mount_ns/ a # 새로 생성한 마운트 지점 확인 $ mount | grep mount_ns tmpfs on /tmp/mount_ns type tmpfs (rw,relatime,seclabel) 새로운 터미널에서 마운트 지점을 확인하면 기존 터미널이 마운트 네임스페이스로 격리되었기 때문에 새로운 터미널에서 보이지 않는 것을 확인할 수 있습니다. bash # 새로운 터미널 # 다른 네임스페이스이기에 네임스페이스의 inode 번호가 다른 것을 확인할 수 있다. $ readlink /proc/$$/ns/mnt mnt:[4026531841] # ls를 사용하더라도 디렉토리는 보이지만 격리된 a 파일은 보이지 않는다. $ ls /tmp/mount_ns/ # 마운트 지점 확인 또한 되지 않는다. $ mount | grep mount_ns UTS 네임스페이스UTS 네임스페이스는 호스트 명을 서버와 다르게 사용할 수 있도록 합니다. UTS는 Unix Time Sharing인 시분할의 약자로 여러 사용자의 요청을 CPU 시간을 쪼개서 사용하며 여러 사용자의 환경 별로 호스트 명이나 도메인 명을 구분할 수 있도록 격리를 제공합니다. bash $ hostname ubuntu1804 $ readlink /proc/$$/ns/uts uts:[4026531838] # 현재 bash 프로세스에서 uts 네임스페이스를 격리 $ sudo unshare --uts /bin/bash # 호스트 명 변경 $ hostname MyHost $ hostname MyHost $ readlink /proc/$$/ns/uts uts:[4026532227] 새로운 터미널에서는 호스트 명이 유지되는 것을 확인할 수 있습니다. bash $ readlink /proc/$$/ns/uts uts:[4026531838] $ hostname ubuntu1804 IPC 네임스페이스IPC 네임스페이스는 Inter Process Communication을 지원하는 공유 메모리 및 메세지 큐 등의 리소스에 대한 격리를 제공합니다. bash $ sudo unshare --ipc /bin/bash # 공유 메모리 생성 $ ipcmk -M 2000 Shared memory id: 0 $ ipcs -m ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0x809712ea 0 root 644 2000 0 $ readlink /proc/$$/ns/ipc ipc:[4026531839] 새로운 터미널에서 공유 메모리를 확인하면 다음과 같습니다. bash # 공유 메모리 생성 $ ipcmk -M 1000 Shared memory id: 0 $ ipcs -m ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0xdf4a6453 0 root 644 1000 0 $ readlink /proc/$$/ns/ipc ipc:[4026532228] PID 네임스페이스pid는 프로세스 별로 부여되는 고유 번호로 프로세스 트리 최상위는 init 프로세스라는 pid 1을 갖는 프로세스가 존재합니다. PID 네임스페이스는 pid 리소스를 격리합니다. PID 네임스페이스를 격리하면 격리된 컨테이너 안에도 pid 1로 시작하는 트리 구조를 가지게 됩니다. PID 네임스페이스는 부모 네임 스페이스와 자식 네임스페이스가 중첩되는 구조로 자식 네임스페이스의 모든 프로세스들은 부모 네임스페이스의 pid와 해당 네임스페이스의 pid를 모두 가지게 됩니다. bash $ echo $$ 646748 $ sudo unshare --pid --fork --mount-proc /bin/bash $ echo $$ 1 // 현재 실행중인 프로세스가 컨테이너의 프로세스만 보인다. $ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.5 233096 5112 pts/1 S 14:24 0:00 /bin/bash root 20 0.0 0.2 232520 2780 pts/1 R+ 14:25 0:00 ps aux cgroup 네임스페이스cgroup은 프로세스에 할당할 시스템 리소스(CPU, Memory, Network)에 대한 제어를 제공합니다. cgroup은 파일시스템을 기반으로 파일시스템에 디렉토리를 만들고 파일을 수정하는 방식으로 시스템 리소스를 관리합니다. 이 때무에 컨테이너에서 cgroup이 관리하는 파일시스템에 접근하게 되면 호스트의 시스템 리소스를 건드릴 수 있습니다. cgroup 네임스페이스는 cgroup 파일시스템(/sys/fs/cgroup)을 격리합니다. 네트워크 네임스페이스네트워크 네임스페이스는 컨테이너의 네트워크를 가상화하여 네트워크 상 별도의 노드로 취급하도록 합니다. bash $ sudo unshare --net /bin/bash $ lsns -t net -p $$ NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026532226 net 2 655964 root unassigned /bin/bash $ lsns -t net -p 1 NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026531840 net 113 1 root unassigned /usr/lib/systemd/systemd --switc NS는 네트워크 네임스페이스의 inode값으로 현재 프로세스와 init 프로세스의 inode 값이 서로 다름을 확인할 수 있습니다. User 네임스페이스User 네임스페이스는 유저가 컨테이너 안에서만 루트 유저로 동작할 수 있도록 합니다. 각 User 네임스페이스는 독립적인 사용자 및 그룹 ID를 할당하여 호스트 시스템의 사용자와 격리된 환경을 제공합니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:1:2","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#ipc-네임스페이스"},{"categories":["DevOps"],"content":" 네임스페이스리눅스 커널의 네임스페이스는 프로세스에 격리된 환경과 리소스를 제공합니다. 네임스페이스는 다음과 같은 특징이 있습니다. 네임스페이스 안에서의 변경은 내부의 프로세스에만 보이고 외부 프로세스에는 보이지 않습니다. 모든 프로세스는 네임스페이스 타입 별로 특정 네임스페이스에 속합니다. 자식 프로세스는 부모 프로세스의 네임스페이스를 상속받습니다. 네임스페이스는 격리하는 자원에 따라 다음과 같이 구분됩니다. 마운트 네임스페이스마운트 네임스페이스는 프로세스와 그 자식 프로세스에게 다른 파일 시스템 마운트 포인트를 제공합니다. 네임스페이스의 프로세스들에게 보여지는 마운트 포인트를 격리시키고 격리된 마운트 포인트들은 각 프로세스에게 단일 디렉토리 구조로 보이게 됩니다. 이때 마운트 네임스페이스의 종류는 다음과 같습니다. private: 각 마운트 포인트가 다른 마운트 포인트에 반영되지 않는 방법 shared: 각 마운트 포인트가 다른 마운트 포인트에 반영되어 보여지는 방법 slave: 파일시스템 하위에서 새로운 마운트는 파일시스템에 반영되나, 반대는 반영되지 않는 방법 bash # 마운트할 디렉터리를 생성 $ mkdir /tmp/mount_ns # 현재 bash 프로세스를 마운트 네임스페이스로 이동 # unshare은 네임스페이스 생성 명령어이고 -m이 마운트 네임스페이스를 생성하도록 지정 $ sudo unshare -m /bin/bash # 현재 네임스페이스의 관련 inode 번호 확인 $ readlink /proc/$$/ns/mnt mnt:[4026532226] # tmpfs이라는 임시 파일 시스템 타입으로 마운트 $ mount -t tmpfs tmpfs /tmp/mount_ns # 임시 파일 생성 $ touch /tmp/mount_ns/a $ ls /tmp/mount_ns/ a # 새로 생성한 마운트 지점 확인 $ mount | grep mount_ns tmpfs on /tmp/mount_ns type tmpfs (rw,relatime,seclabel) 새로운 터미널에서 마운트 지점을 확인하면 기존 터미널이 마운트 네임스페이스로 격리되었기 때문에 새로운 터미널에서 보이지 않는 것을 확인할 수 있습니다. bash # 새로운 터미널 # 다른 네임스페이스이기에 네임스페이스의 inode 번호가 다른 것을 확인할 수 있다. $ readlink /proc/$$/ns/mnt mnt:[4026531841] # ls를 사용하더라도 디렉토리는 보이지만 격리된 a 파일은 보이지 않는다. $ ls /tmp/mount_ns/ # 마운트 지점 확인 또한 되지 않는다. $ mount | grep mount_ns UTS 네임스페이스UTS 네임스페이스는 호스트 명을 서버와 다르게 사용할 수 있도록 합니다. UTS는 Unix Time Sharing인 시분할의 약자로 여러 사용자의 요청을 CPU 시간을 쪼개서 사용하며 여러 사용자의 환경 별로 호스트 명이나 도메인 명을 구분할 수 있도록 격리를 제공합니다. bash $ hostname ubuntu1804 $ readlink /proc/$$/ns/uts uts:[4026531838] # 현재 bash 프로세스에서 uts 네임스페이스를 격리 $ sudo unshare --uts /bin/bash # 호스트 명 변경 $ hostname MyHost $ hostname MyHost $ readlink /proc/$$/ns/uts uts:[4026532227] 새로운 터미널에서는 호스트 명이 유지되는 것을 확인할 수 있습니다. bash $ readlink /proc/$$/ns/uts uts:[4026531838] $ hostname ubuntu1804 IPC 네임스페이스IPC 네임스페이스는 Inter Process Communication을 지원하는 공유 메모리 및 메세지 큐 등의 리소스에 대한 격리를 제공합니다. bash $ sudo unshare --ipc /bin/bash # 공유 메모리 생성 $ ipcmk -M 2000 Shared memory id: 0 $ ipcs -m ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0x809712ea 0 root 644 2000 0 $ readlink /proc/$$/ns/ipc ipc:[4026531839] 새로운 터미널에서 공유 메모리를 확인하면 다음과 같습니다. bash # 공유 메모리 생성 $ ipcmk -M 1000 Shared memory id: 0 $ ipcs -m ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0xdf4a6453 0 root 644 1000 0 $ readlink /proc/$$/ns/ipc ipc:[4026532228] PID 네임스페이스pid는 프로세스 별로 부여되는 고유 번호로 프로세스 트리 최상위는 init 프로세스라는 pid 1을 갖는 프로세스가 존재합니다. PID 네임스페이스는 pid 리소스를 격리합니다. PID 네임스페이스를 격리하면 격리된 컨테이너 안에도 pid 1로 시작하는 트리 구조를 가지게 됩니다. PID 네임스페이스는 부모 네임 스페이스와 자식 네임스페이스가 중첩되는 구조로 자식 네임스페이스의 모든 프로세스들은 부모 네임스페이스의 pid와 해당 네임스페이스의 pid를 모두 가지게 됩니다. bash $ echo $$ 646748 $ sudo unshare --pid --fork --mount-proc /bin/bash $ echo $$ 1 // 현재 실행중인 프로세스가 컨테이너의 프로세스만 보인다. $ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.5 233096 5112 pts/1 S 14:24 0:00 /bin/bash root 20 0.0 0.2 232520 2780 pts/1 R+ 14:25 0:00 ps aux cgroup 네임스페이스cgroup은 프로세스에 할당할 시스템 리소스(CPU, Memory, Network)에 대한 제어를 제공합니다. cgroup은 파일시스템을 기반으로 파일시스템에 디렉토리를 만들고 파일을 수정하는 방식으로 시스템 리소스를 관리합니다. 이 때무에 컨테이너에서 cgroup이 관리하는 파일시스템에 접근하게 되면 호스트의 시스템 리소스를 건드릴 수 있습니다. cgroup 네임스페이스는 cgroup 파일시스템(/sys/fs/cgroup)을 격리합니다. 네트워크 네임스페이스네트워크 네임스페이스는 컨테이너의 네트워크를 가상화하여 네트워크 상 별도의 노드로 취급하도록 합니다. bash $ sudo unshare --net /bin/bash $ lsns -t net -p $$ NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026532226 net 2 655964 root unassigned /bin/bash $ lsns -t net -p 1 NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026531840 net 113 1 root unassigned /usr/lib/systemd/systemd --switc NS는 네트워크 네임스페이스의 inode값으로 현재 프로세스와 init 프로세스의 inode 값이 서로 다름을 확인할 수 있습니다. User 네임스페이스User 네임스페이스는 유저가 컨테이너 안에서만 루트 유저로 동작할 수 있도록 합니다. 각 User 네임스페이스는 독립적인 사용자 및 그룹 ID를 할당하여 호스트 시스템의 사용자와 격리된 환경을 제공합니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:1:2","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#pid-네임스페이스"},{"categories":["DevOps"],"content":" 네임스페이스리눅스 커널의 네임스페이스는 프로세스에 격리된 환경과 리소스를 제공합니다. 네임스페이스는 다음과 같은 특징이 있습니다. 네임스페이스 안에서의 변경은 내부의 프로세스에만 보이고 외부 프로세스에는 보이지 않습니다. 모든 프로세스는 네임스페이스 타입 별로 특정 네임스페이스에 속합니다. 자식 프로세스는 부모 프로세스의 네임스페이스를 상속받습니다. 네임스페이스는 격리하는 자원에 따라 다음과 같이 구분됩니다. 마운트 네임스페이스마운트 네임스페이스는 프로세스와 그 자식 프로세스에게 다른 파일 시스템 마운트 포인트를 제공합니다. 네임스페이스의 프로세스들에게 보여지는 마운트 포인트를 격리시키고 격리된 마운트 포인트들은 각 프로세스에게 단일 디렉토리 구조로 보이게 됩니다. 이때 마운트 네임스페이스의 종류는 다음과 같습니다. private: 각 마운트 포인트가 다른 마운트 포인트에 반영되지 않는 방법 shared: 각 마운트 포인트가 다른 마운트 포인트에 반영되어 보여지는 방법 slave: 파일시스템 하위에서 새로운 마운트는 파일시스템에 반영되나, 반대는 반영되지 않는 방법 bash # 마운트할 디렉터리를 생성 $ mkdir /tmp/mount_ns # 현재 bash 프로세스를 마운트 네임스페이스로 이동 # unshare은 네임스페이스 생성 명령어이고 -m이 마운트 네임스페이스를 생성하도록 지정 $ sudo unshare -m /bin/bash # 현재 네임스페이스의 관련 inode 번호 확인 $ readlink /proc/$$/ns/mnt mnt:[4026532226] # tmpfs이라는 임시 파일 시스템 타입으로 마운트 $ mount -t tmpfs tmpfs /tmp/mount_ns # 임시 파일 생성 $ touch /tmp/mount_ns/a $ ls /tmp/mount_ns/ a # 새로 생성한 마운트 지점 확인 $ mount | grep mount_ns tmpfs on /tmp/mount_ns type tmpfs (rw,relatime,seclabel) 새로운 터미널에서 마운트 지점을 확인하면 기존 터미널이 마운트 네임스페이스로 격리되었기 때문에 새로운 터미널에서 보이지 않는 것을 확인할 수 있습니다. bash # 새로운 터미널 # 다른 네임스페이스이기에 네임스페이스의 inode 번호가 다른 것을 확인할 수 있다. $ readlink /proc/$$/ns/mnt mnt:[4026531841] # ls를 사용하더라도 디렉토리는 보이지만 격리된 a 파일은 보이지 않는다. $ ls /tmp/mount_ns/ # 마운트 지점 확인 또한 되지 않는다. $ mount | grep mount_ns UTS 네임스페이스UTS 네임스페이스는 호스트 명을 서버와 다르게 사용할 수 있도록 합니다. UTS는 Unix Time Sharing인 시분할의 약자로 여러 사용자의 요청을 CPU 시간을 쪼개서 사용하며 여러 사용자의 환경 별로 호스트 명이나 도메인 명을 구분할 수 있도록 격리를 제공합니다. bash $ hostname ubuntu1804 $ readlink /proc/$$/ns/uts uts:[4026531838] # 현재 bash 프로세스에서 uts 네임스페이스를 격리 $ sudo unshare --uts /bin/bash # 호스트 명 변경 $ hostname MyHost $ hostname MyHost $ readlink /proc/$$/ns/uts uts:[4026532227] 새로운 터미널에서는 호스트 명이 유지되는 것을 확인할 수 있습니다. bash $ readlink /proc/$$/ns/uts uts:[4026531838] $ hostname ubuntu1804 IPC 네임스페이스IPC 네임스페이스는 Inter Process Communication을 지원하는 공유 메모리 및 메세지 큐 등의 리소스에 대한 격리를 제공합니다. bash $ sudo unshare --ipc /bin/bash # 공유 메모리 생성 $ ipcmk -M 2000 Shared memory id: 0 $ ipcs -m ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0x809712ea 0 root 644 2000 0 $ readlink /proc/$$/ns/ipc ipc:[4026531839] 새로운 터미널에서 공유 메모리를 확인하면 다음과 같습니다. bash # 공유 메모리 생성 $ ipcmk -M 1000 Shared memory id: 0 $ ipcs -m ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0xdf4a6453 0 root 644 1000 0 $ readlink /proc/$$/ns/ipc ipc:[4026532228] PID 네임스페이스pid는 프로세스 별로 부여되는 고유 번호로 프로세스 트리 최상위는 init 프로세스라는 pid 1을 갖는 프로세스가 존재합니다. PID 네임스페이스는 pid 리소스를 격리합니다. PID 네임스페이스를 격리하면 격리된 컨테이너 안에도 pid 1로 시작하는 트리 구조를 가지게 됩니다. PID 네임스페이스는 부모 네임 스페이스와 자식 네임스페이스가 중첩되는 구조로 자식 네임스페이스의 모든 프로세스들은 부모 네임스페이스의 pid와 해당 네임스페이스의 pid를 모두 가지게 됩니다. bash $ echo $$ 646748 $ sudo unshare --pid --fork --mount-proc /bin/bash $ echo $$ 1 // 현재 실행중인 프로세스가 컨테이너의 프로세스만 보인다. $ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.5 233096 5112 pts/1 S 14:24 0:00 /bin/bash root 20 0.0 0.2 232520 2780 pts/1 R+ 14:25 0:00 ps aux cgroup 네임스페이스cgroup은 프로세스에 할당할 시스템 리소스(CPU, Memory, Network)에 대한 제어를 제공합니다. cgroup은 파일시스템을 기반으로 파일시스템에 디렉토리를 만들고 파일을 수정하는 방식으로 시스템 리소스를 관리합니다. 이 때무에 컨테이너에서 cgroup이 관리하는 파일시스템에 접근하게 되면 호스트의 시스템 리소스를 건드릴 수 있습니다. cgroup 네임스페이스는 cgroup 파일시스템(/sys/fs/cgroup)을 격리합니다. 네트워크 네임스페이스네트워크 네임스페이스는 컨테이너의 네트워크를 가상화하여 네트워크 상 별도의 노드로 취급하도록 합니다. bash $ sudo unshare --net /bin/bash $ lsns -t net -p $$ NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026532226 net 2 655964 root unassigned /bin/bash $ lsns -t net -p 1 NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026531840 net 113 1 root unassigned /usr/lib/systemd/systemd --switc NS는 네트워크 네임스페이스의 inode값으로 현재 프로세스와 init 프로세스의 inode 값이 서로 다름을 확인할 수 있습니다. User 네임스페이스User 네임스페이스는 유저가 컨테이너 안에서만 루트 유저로 동작할 수 있도록 합니다. 각 User 네임스페이스는 독립적인 사용자 및 그룹 ID를 할당하여 호스트 시스템의 사용자와 격리된 환경을 제공합니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:1:2","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#cgroup-네임스페이스"},{"categories":["DevOps"],"content":" 네임스페이스리눅스 커널의 네임스페이스는 프로세스에 격리된 환경과 리소스를 제공합니다. 네임스페이스는 다음과 같은 특징이 있습니다. 네임스페이스 안에서의 변경은 내부의 프로세스에만 보이고 외부 프로세스에는 보이지 않습니다. 모든 프로세스는 네임스페이스 타입 별로 특정 네임스페이스에 속합니다. 자식 프로세스는 부모 프로세스의 네임스페이스를 상속받습니다. 네임스페이스는 격리하는 자원에 따라 다음과 같이 구분됩니다. 마운트 네임스페이스마운트 네임스페이스는 프로세스와 그 자식 프로세스에게 다른 파일 시스템 마운트 포인트를 제공합니다. 네임스페이스의 프로세스들에게 보여지는 마운트 포인트를 격리시키고 격리된 마운트 포인트들은 각 프로세스에게 단일 디렉토리 구조로 보이게 됩니다. 이때 마운트 네임스페이스의 종류는 다음과 같습니다. private: 각 마운트 포인트가 다른 마운트 포인트에 반영되지 않는 방법 shared: 각 마운트 포인트가 다른 마운트 포인트에 반영되어 보여지는 방법 slave: 파일시스템 하위에서 새로운 마운트는 파일시스템에 반영되나, 반대는 반영되지 않는 방법 bash # 마운트할 디렉터리를 생성 $ mkdir /tmp/mount_ns # 현재 bash 프로세스를 마운트 네임스페이스로 이동 # unshare은 네임스페이스 생성 명령어이고 -m이 마운트 네임스페이스를 생성하도록 지정 $ sudo unshare -m /bin/bash # 현재 네임스페이스의 관련 inode 번호 확인 $ readlink /proc/$$/ns/mnt mnt:[4026532226] # tmpfs이라는 임시 파일 시스템 타입으로 마운트 $ mount -t tmpfs tmpfs /tmp/mount_ns # 임시 파일 생성 $ touch /tmp/mount_ns/a $ ls /tmp/mount_ns/ a # 새로 생성한 마운트 지점 확인 $ mount | grep mount_ns tmpfs on /tmp/mount_ns type tmpfs (rw,relatime,seclabel) 새로운 터미널에서 마운트 지점을 확인하면 기존 터미널이 마운트 네임스페이스로 격리되었기 때문에 새로운 터미널에서 보이지 않는 것을 확인할 수 있습니다. bash # 새로운 터미널 # 다른 네임스페이스이기에 네임스페이스의 inode 번호가 다른 것을 확인할 수 있다. $ readlink /proc/$$/ns/mnt mnt:[4026531841] # ls를 사용하더라도 디렉토리는 보이지만 격리된 a 파일은 보이지 않는다. $ ls /tmp/mount_ns/ # 마운트 지점 확인 또한 되지 않는다. $ mount | grep mount_ns UTS 네임스페이스UTS 네임스페이스는 호스트 명을 서버와 다르게 사용할 수 있도록 합니다. UTS는 Unix Time Sharing인 시분할의 약자로 여러 사용자의 요청을 CPU 시간을 쪼개서 사용하며 여러 사용자의 환경 별로 호스트 명이나 도메인 명을 구분할 수 있도록 격리를 제공합니다. bash $ hostname ubuntu1804 $ readlink /proc/$$/ns/uts uts:[4026531838] # 현재 bash 프로세스에서 uts 네임스페이스를 격리 $ sudo unshare --uts /bin/bash # 호스트 명 변경 $ hostname MyHost $ hostname MyHost $ readlink /proc/$$/ns/uts uts:[4026532227] 새로운 터미널에서는 호스트 명이 유지되는 것을 확인할 수 있습니다. bash $ readlink /proc/$$/ns/uts uts:[4026531838] $ hostname ubuntu1804 IPC 네임스페이스IPC 네임스페이스는 Inter Process Communication을 지원하는 공유 메모리 및 메세지 큐 등의 리소스에 대한 격리를 제공합니다. bash $ sudo unshare --ipc /bin/bash # 공유 메모리 생성 $ ipcmk -M 2000 Shared memory id: 0 $ ipcs -m ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0x809712ea 0 root 644 2000 0 $ readlink /proc/$$/ns/ipc ipc:[4026531839] 새로운 터미널에서 공유 메모리를 확인하면 다음과 같습니다. bash # 공유 메모리 생성 $ ipcmk -M 1000 Shared memory id: 0 $ ipcs -m ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0xdf4a6453 0 root 644 1000 0 $ readlink /proc/$$/ns/ipc ipc:[4026532228] PID 네임스페이스pid는 프로세스 별로 부여되는 고유 번호로 프로세스 트리 최상위는 init 프로세스라는 pid 1을 갖는 프로세스가 존재합니다. PID 네임스페이스는 pid 리소스를 격리합니다. PID 네임스페이스를 격리하면 격리된 컨테이너 안에도 pid 1로 시작하는 트리 구조를 가지게 됩니다. PID 네임스페이스는 부모 네임 스페이스와 자식 네임스페이스가 중첩되는 구조로 자식 네임스페이스의 모든 프로세스들은 부모 네임스페이스의 pid와 해당 네임스페이스의 pid를 모두 가지게 됩니다. bash $ echo $$ 646748 $ sudo unshare --pid --fork --mount-proc /bin/bash $ echo $$ 1 // 현재 실행중인 프로세스가 컨테이너의 프로세스만 보인다. $ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.5 233096 5112 pts/1 S 14:24 0:00 /bin/bash root 20 0.0 0.2 232520 2780 pts/1 R+ 14:25 0:00 ps aux cgroup 네임스페이스cgroup은 프로세스에 할당할 시스템 리소스(CPU, Memory, Network)에 대한 제어를 제공합니다. cgroup은 파일시스템을 기반으로 파일시스템에 디렉토리를 만들고 파일을 수정하는 방식으로 시스템 리소스를 관리합니다. 이 때무에 컨테이너에서 cgroup이 관리하는 파일시스템에 접근하게 되면 호스트의 시스템 리소스를 건드릴 수 있습니다. cgroup 네임스페이스는 cgroup 파일시스템(/sys/fs/cgroup)을 격리합니다. 네트워크 네임스페이스네트워크 네임스페이스는 컨테이너의 네트워크를 가상화하여 네트워크 상 별도의 노드로 취급하도록 합니다. bash $ sudo unshare --net /bin/bash $ lsns -t net -p $$ NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026532226 net 2 655964 root unassigned /bin/bash $ lsns -t net -p 1 NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026531840 net 113 1 root unassigned /usr/lib/systemd/systemd --switc NS는 네트워크 네임스페이스의 inode값으로 현재 프로세스와 init 프로세스의 inode 값이 서로 다름을 확인할 수 있습니다. User 네임스페이스User 네임스페이스는 유저가 컨테이너 안에서만 루트 유저로 동작할 수 있도록 합니다. 각 User 네임스페이스는 독립적인 사용자 및 그룹 ID를 할당하여 호스트 시스템의 사용자와 격리된 환경을 제공합니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:1:2","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#네트워크-네임스페이스"},{"categories":["DevOps"],"content":" 네임스페이스리눅스 커널의 네임스페이스는 프로세스에 격리된 환경과 리소스를 제공합니다. 네임스페이스는 다음과 같은 특징이 있습니다. 네임스페이스 안에서의 변경은 내부의 프로세스에만 보이고 외부 프로세스에는 보이지 않습니다. 모든 프로세스는 네임스페이스 타입 별로 특정 네임스페이스에 속합니다. 자식 프로세스는 부모 프로세스의 네임스페이스를 상속받습니다. 네임스페이스는 격리하는 자원에 따라 다음과 같이 구분됩니다. 마운트 네임스페이스마운트 네임스페이스는 프로세스와 그 자식 프로세스에게 다른 파일 시스템 마운트 포인트를 제공합니다. 네임스페이스의 프로세스들에게 보여지는 마운트 포인트를 격리시키고 격리된 마운트 포인트들은 각 프로세스에게 단일 디렉토리 구조로 보이게 됩니다. 이때 마운트 네임스페이스의 종류는 다음과 같습니다. private: 각 마운트 포인트가 다른 마운트 포인트에 반영되지 않는 방법 shared: 각 마운트 포인트가 다른 마운트 포인트에 반영되어 보여지는 방법 slave: 파일시스템 하위에서 새로운 마운트는 파일시스템에 반영되나, 반대는 반영되지 않는 방법 bash # 마운트할 디렉터리를 생성 $ mkdir /tmp/mount_ns # 현재 bash 프로세스를 마운트 네임스페이스로 이동 # unshare은 네임스페이스 생성 명령어이고 -m이 마운트 네임스페이스를 생성하도록 지정 $ sudo unshare -m /bin/bash # 현재 네임스페이스의 관련 inode 번호 확인 $ readlink /proc/$$/ns/mnt mnt:[4026532226] # tmpfs이라는 임시 파일 시스템 타입으로 마운트 $ mount -t tmpfs tmpfs /tmp/mount_ns # 임시 파일 생성 $ touch /tmp/mount_ns/a $ ls /tmp/mount_ns/ a # 새로 생성한 마운트 지점 확인 $ mount | grep mount_ns tmpfs on /tmp/mount_ns type tmpfs (rw,relatime,seclabel) 새로운 터미널에서 마운트 지점을 확인하면 기존 터미널이 마운트 네임스페이스로 격리되었기 때문에 새로운 터미널에서 보이지 않는 것을 확인할 수 있습니다. bash # 새로운 터미널 # 다른 네임스페이스이기에 네임스페이스의 inode 번호가 다른 것을 확인할 수 있다. $ readlink /proc/$$/ns/mnt mnt:[4026531841] # ls를 사용하더라도 디렉토리는 보이지만 격리된 a 파일은 보이지 않는다. $ ls /tmp/mount_ns/ # 마운트 지점 확인 또한 되지 않는다. $ mount | grep mount_ns UTS 네임스페이스UTS 네임스페이스는 호스트 명을 서버와 다르게 사용할 수 있도록 합니다. UTS는 Unix Time Sharing인 시분할의 약자로 여러 사용자의 요청을 CPU 시간을 쪼개서 사용하며 여러 사용자의 환경 별로 호스트 명이나 도메인 명을 구분할 수 있도록 격리를 제공합니다. bash $ hostname ubuntu1804 $ readlink /proc/$$/ns/uts uts:[4026531838] # 현재 bash 프로세스에서 uts 네임스페이스를 격리 $ sudo unshare --uts /bin/bash # 호스트 명 변경 $ hostname MyHost $ hostname MyHost $ readlink /proc/$$/ns/uts uts:[4026532227] 새로운 터미널에서는 호스트 명이 유지되는 것을 확인할 수 있습니다. bash $ readlink /proc/$$/ns/uts uts:[4026531838] $ hostname ubuntu1804 IPC 네임스페이스IPC 네임스페이스는 Inter Process Communication을 지원하는 공유 메모리 및 메세지 큐 등의 리소스에 대한 격리를 제공합니다. bash $ sudo unshare --ipc /bin/bash # 공유 메모리 생성 $ ipcmk -M 2000 Shared memory id: 0 $ ipcs -m ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0x809712ea 0 root 644 2000 0 $ readlink /proc/$$/ns/ipc ipc:[4026531839] 새로운 터미널에서 공유 메모리를 확인하면 다음과 같습니다. bash # 공유 메모리 생성 $ ipcmk -M 1000 Shared memory id: 0 $ ipcs -m ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0xdf4a6453 0 root 644 1000 0 $ readlink /proc/$$/ns/ipc ipc:[4026532228] PID 네임스페이스pid는 프로세스 별로 부여되는 고유 번호로 프로세스 트리 최상위는 init 프로세스라는 pid 1을 갖는 프로세스가 존재합니다. PID 네임스페이스는 pid 리소스를 격리합니다. PID 네임스페이스를 격리하면 격리된 컨테이너 안에도 pid 1로 시작하는 트리 구조를 가지게 됩니다. PID 네임스페이스는 부모 네임 스페이스와 자식 네임스페이스가 중첩되는 구조로 자식 네임스페이스의 모든 프로세스들은 부모 네임스페이스의 pid와 해당 네임스페이스의 pid를 모두 가지게 됩니다. bash $ echo $$ 646748 $ sudo unshare --pid --fork --mount-proc /bin/bash $ echo $$ 1 // 현재 실행중인 프로세스가 컨테이너의 프로세스만 보인다. $ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.5 233096 5112 pts/1 S 14:24 0:00 /bin/bash root 20 0.0 0.2 232520 2780 pts/1 R+ 14:25 0:00 ps aux cgroup 네임스페이스cgroup은 프로세스에 할당할 시스템 리소스(CPU, Memory, Network)에 대한 제어를 제공합니다. cgroup은 파일시스템을 기반으로 파일시스템에 디렉토리를 만들고 파일을 수정하는 방식으로 시스템 리소스를 관리합니다. 이 때무에 컨테이너에서 cgroup이 관리하는 파일시스템에 접근하게 되면 호스트의 시스템 리소스를 건드릴 수 있습니다. cgroup 네임스페이스는 cgroup 파일시스템(/sys/fs/cgroup)을 격리합니다. 네트워크 네임스페이스네트워크 네임스페이스는 컨테이너의 네트워크를 가상화하여 네트워크 상 별도의 노드로 취급하도록 합니다. bash $ sudo unshare --net /bin/bash $ lsns -t net -p $$ NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026532226 net 2 655964 root unassigned /bin/bash $ lsns -t net -p 1 NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026531840 net 113 1 root unassigned /usr/lib/systemd/systemd --switc NS는 네트워크 네임스페이스의 inode값으로 현재 프로세스와 init 프로세스의 inode 값이 서로 다름을 확인할 수 있습니다. User 네임스페이스User 네임스페이스는 유저가 컨테이너 안에서만 루트 유저로 동작할 수 있도록 합니다. 각 User 네임스페이스는 독립적인 사용자 및 그룹 ID를 할당하여 호스트 시스템의 사용자와 격리된 환경을 제공합니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:1:2","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#user-네임스페이스"},{"categories":["DevOps"],"content":" 컨테이너 계층화컨테이너 이미지는 컨테이너 내부에서 프로그램을 실행하는데 필요한 모든 파일을 캡슐화하는 바이너리 패키지입니다. 이미지는 일련의 파일 시스템 계층으로 구성되어있어 각 계층은 파일 시스템의 이전 계층으로부터 파일을 추가, 제거 또는 수정하는 오버레이 파일 시스템을 갖습니다. 컨테이너 이미지는 일련의 파일 시스템 계층으로 구성되어 있어 이전 계층을 상속하고 수정합니다. 도커의 모든 이미지 레이어는 var/lib/docker/overlay2에 저장되고 해시값을 통해 구분된다. Dockerfile의 FROM, RUN, COPY 명령은 새로운 레이어를 생성합니다. Example 이미지 A: 레이어 A -\u003e 레이어 B -\u003e 레이어 C 이미지 B: 레이어 A -\u003e 레이어 B -\u003e 레이어 C -\u003e 레이어 D 이미지 C: 레이어 A -\u003e 레이어 B -\u003e 레이어 E 위와 같이 이미지 3개가 존재한다고 할 때, 이미지 A를 삭제하더라도 레이어 A, B, C는 이미지 B에서 사용되고 있기 때문에 파일 시스템에서 지워지지 않고, 이미지 C를 추가로 생성할 때에 레이어 A와 B가 이미 다운로드 되어있기에 별도로 다운로드하거나 생성하지 않고 레이어 E만 만들게 됩니다. Dockerfile # Dockerfile A FROM ubuntu RUN apt-get update \u0026\u0026 apt-get upgrade -y RUN apt-get install -y openssh-server Dockerfile # Dockerfile B FROM ubuntu RUN apt-get update \u0026\u0026 apt-get upgrade -y \u0026\u0026 apt-get install -y openssh-server 만약 위와 같이 2개의 이미지가 존재한다면, 해당 이미지를 docker inspect 명령어를 통하여 보았을 때 레이어를 파악할 수 있습니다. bash $ docker inspect image_a \"RootFS\": { \"Type\": \"layers\", \"Layers\": [ \"sha256:17ccff5b06b404f920af4557c0e2cb6ee5551b7136dc0cdb4f9aaf1bf71c643c\", \"sha256:ede334733a68a6091f2137efcdc0b86284885442db0c96cb72b0c47b27d74ed6\", \"sha256:47592da01660e3fb88cfbb307cbcd8faf61b879bf6fc2eaccae598d0a35ab53b\" ] }, bash $ docker inspect image_b \"RootFS\": { \"Type\": \"layers\", \"Layers\": [ \"sha256:17ccff5b06b404f920af4557c0e2cb6ee5551b7136dc0cdb4f9aaf1bf71c643c\", \"sha256:d1bd4cf291b49e133ee9c9f4679f45fffe91e2cfca7a048466d67eebd7bef269\" ] }, 위와 같이 기반 이미지의 레이어인 sha256:17ccff5b06b404f920af4557c0e2cb6ee5551b7136dc0cdb4f9aaf1bf71c643c을 공통으로 갖고 실행된 RUN 명령어마다 새로운 레이어가 추가되는 것을 확인할 수 있습니다. ","date":"2024-07-11","objectID":"/posts/k8s-container/:2:0","series":null,"tags":["container","virtualization"],"title":"[DevOps] 컨테이너란?","uri":"/posts/k8s-container/#컨테이너-계층화"},{"categories":["python"],"content":"Python의 Global Interpreter Lock(GIL)은 여러 스레드가 동시에 Python 코드를 실행하는 것을 막는 기법입니다.","date":"2024-07-11","objectID":"/posts/python-gil/","series":null,"tags":["GIL","python","multithreading"],"title":"[Python] GIL","uri":"/posts/python-gil/"},{"categories":["python"],"content":" 멀티 스레딩멀티 스레딩은 하나의 프로세스 내에서 여러 스레드를 동시에 실행하여 작업을 병렬로 처리하는 프로그래밍 기법입니다. 각각의 스레드는 독립적으로 실행되지만, 같은 프로세스 내의 자원을 공유하므로 데이터를 효율적으로 교환하고, 병렬 작업을 처리할 수 있습니다. ","date":"2024-07-11","objectID":"/posts/python-gil/:1:0","series":null,"tags":["GIL","python","multithreading"],"title":"[Python] GIL","uri":"/posts/python-gil/#멀티-스레딩"},{"categories":["python"],"content":" Race Condition하지만, 멀티 스레딩은 자원을 공유하는 만큼 공유된 데이터 처리에 대한 고려가 필요합니다. 여러 스레드가 동시에 한 자원에 접근하게 되면 접근 순서에 따라 실행 결과가 다르거나 의도하지 않은 결과가 발생할 수 있고 이를 Race Condition이라고 합니다. 다음은 Race Condition의 간단한 예시입니다. Race Condition 예시 계좌의 잔액을 확인하고 현재 계좌에 500원만큼 입금하는 함수가 있습니다. 계좌는 잔액이 1000원으로 시작하고 두 개의 스레드가 동시에 입금 함수를 실행하려고 합니다. 첫 번째 스레드는 현재 잔액이 1000원임을 확인하고 잔액을 500원이 추가된 1500원으로 변경하려고 합니다. 이때, 첫번째 스레드가 1500원으로 변경하기 전에 두번째 스레드가 접근하여 현재 잔액이 1000원임을 확인하고 잔액을 1500원으로 변경합니다. 이후 첫번째 스레드는 잔액을 1500원으로 변경합니다. 이러면 실제로 2000원이 되어야 하지만 1500원이 되는 문제가 발생하게됩니다. ","date":"2024-07-11","objectID":"/posts/python-gil/:1:1","series":null,"tags":["GIL","python","multithreading"],"title":"[Python] GIL","uri":"/posts/python-gil/#race-condition"},{"categories":["python"],"content":" Python에서의 멀티 스레딩Python에서도 멀티 스레딩을 지원하지만 Python의 GC 방식인 Reference Count 때문에 문제가 발생하게 됩니다. Python의 모든 객체는 reference count 값을 가지고 객체의 할당과 해제 시에 해당 값을 관리하게 됩니다. [Python] Garbage Collection 가비지 컬렉션은 프로그래밍 언어에서 더 이상 필요하지 않은 객체를 자동으로 메모리에서 해제하는 메모리 관리 방법입니다. 그렇다면 Python에서는 어떻게 GC를 처리할까요? 더 읽기... Python에서 멀티 스레딩을 사용하게 되면 한 객체에 여러 스레드가 접근할 수 있게 되고, 이 reference count값을 변경할 때에 Race Condition이 발생할 수 있는 문제가 있습니다. reference count값을 잘못 관리하면 객체가 삭제되지 않아 메모리 누수가 발생하거나 객체에 대한 참조가 있음에도 객체가 삭제되는 문제가 발생할 수도 있어 Python은 GIL을 통해 해당 문제를 막도록 하였습니다. ","date":"2024-07-11","objectID":"/posts/python-gil/:2:0","series":null,"tags":["GIL","python","multithreading"],"title":"[Python] GIL","uri":"/posts/python-gil/#python에서의-멀티-스레딩"},{"categories":["python"],"content":" Python GILPython의 Global Interpreter Lock(GIL)은 여러 스레드가 동시에 Python 코드를 실행하는 것을 막는 기법입니다. 즉, 동시에 오직 하나의 스레드만 실행될 수 있다는 것입니다. multithreading_cpu_bound.py import threading import time def cpu_bound_task(): count = 0 while count \u003c 10**7: count += 1 def log_time(func): def wrapper(*args, **kwargs): start_time = time.time() result = func(*args, **kwargs) end_time = time.time() print(f\"Execution time: {end_time - start_time:.4f} seconds\") return result return wrapper @log_time def run_in_threads(): thread1 = threading.Thread(target=cpu_bound_task) thread2 = threading.Thread(target=cpu_bound_task) thread1.start() thread2.start() thread1.join() thread2.join() @log_time def run_sequentially(): cpu_bound_task() cpu_bound_task() print(\"Running in threads:\") run_in_threads() print(\"\\nRunning sequentially:\") run_sequentially() 위 코드는 수를 천만까지 세는 함수 2개를 멀티 스레드 환경에서 병렬로 실행한 것과 순차적으로 실행한 것의 결과를 비교하는 예시입니다. 다음와 같이 멀티 스레딩과 순차적 실행에 걸리는 시간의 차이가 거의 없는 것을 확인할 수 있습니다. bash $ python multithreading_cpu_bound.py Running in threads: Execution time: 0.4209 seconds Running sequentially: Execution time: 0.4320 seconds 그렇다면, Python에서 멀티 스레딩은 병렬로 처리되지도 않고 Context Switching 오버헤드만 발생할텐데 불필요한 것일까요? 그것은 아닙니다. Python이 GIL을 사용하는 이유는 스레드가 동시에 공유 자원에 접근하는 것을 막기 위함이기에 동시에 여러 CPU가 접근하는 것은 막지만, 파일 시스템에서 데이터를 읽고 저장하는 것과 같은 I/O 작업에 대해서는 막지 않습니다. 즉, Python의 멀티 스레딩은 CPU Bound 작업에 대해서는 성능의 향상이 없지만 I/O Bound 작업에는 병렬 처리가 가능합니다. multithreading_io_bound.py import threading import time def io_bound_task(task_name, delay): time.sleep(delay) def log_time(func): def wrapper(*args, **kwargs): start_time = time.time() result = func(*args, **kwargs) end_time = time.time() print(f\"Execution time: {end_time - start_time:.4f} seconds\") return result return wrapper @log_time def run_in_threads(): thread1 = threading.Thread(target=io_bound_task, args=(\"Task 1\", 2)) thread2 = threading.Thread(target=io_bound_task, args=(\"Task 2\", 3)) thread1.start() thread2.start() thread1.join() thread2.join() @log_time def run_sequentially(): io_bound_task(\"Task 1\", 2) io_bound_task(\"Task 2\", 3) print(\"Running in threads:\") run_in_threads() print(\"\\nRunning sequentially:\") run_sequentially() 위 코드는 I/O 작업인 sleep 함수를 2개를 멀티 스레드 환경에서 병렬로 실행한 것과 순차적으로 실행한 것의 결과를 비교하는 예시입니다. 다음와 같이 멀티 스레딩에서는 순차적 실행과 달리 함수가 병렬로 실행된 것을 확인할 수 있습니다. bash $ python multithreading_io_bound.py Running in threads: Execution time: 3.0037 seconds Running sequentially: Execution time: 5.0084 seconds 따라서 Python에서는 CPU Bound 작업에 대해서는 멀티 프로세싱을, I/O Bound 작업에 대해서는 멀티 스레딩을 사용하는 것을 권장합니다. ","date":"2024-07-11","objectID":"/posts/python-gil/:3:0","series":null,"tags":["GIL","python","multithreading"],"title":"[Python] GIL","uri":"/posts/python-gil/#python-gil"},{"categories":["python"],"content":" GIL 문제점 해결하지만 Python 개발자들은 이러한 GIL의 문제점에 대해서 꾸준히 문제점을 제기했고 결국 Python은 GIL의 문제점을 해결하기 위해 노력하고 있습니다. ","date":"2024-07-11","objectID":"/posts/python-gil/:4:0","series":null,"tags":["GIL","python","multithreading"],"title":"[Python] GIL","uri":"/posts/python-gil/#gil-문제점-해결"},{"categories":["python"],"content":" 서브 인터프리터를 통한 병렬 처리2023년 10월 2일 출시된 Python 3.12부터 서브 인터프리터 별로 개별적인 GIL을 생성하도록 업데이트 되었습니다. 즉, Python 3.12부터 서브 인터프리터 별로 개별 GIL을 생성하며 CPU Bound 작업에서의 병렬 처리도 가능해졌지만 서브 인터프리터는 아직 C-API만을 제공하고 있고 Python 3.13부터 서브 인터프리터를 표준 라이브러리에 포함하려고 하고 있습니다. Python 서브 인터프리터란 메인 인터프리터와 동일한 프로세스와 동일한 주소 공간을 사용하여 병렬로 Python 코드를 실행하지만 완전히 독립된 자원으로 실행되는 인터프리터입니다. 즉, OS로 부터 할당받은 프로세스의 메모리 공간을 각각의 서브 인터프리터가 독립적으로 사용하는 방식입니다. 서브 인터프리터 vs 멀티 스레딩멀티 스레딩과 비교하면 멀티 스레드는 스레드끼리 동일한 메모리 공간을 사용하기 때문에 자원 공유가 편하지만 공유 자원에 race condition이 발생할 수 있어 동기화에 대한 고려가 필요합니다. 반면 서브 인터프리터는 메모리 공간이 독립적이기에 자원 공유를 위해 별도의 채널을 사용해야하여 자원 공유 속도가 스레드에 비하여 느리지만 원전히 독립적으로 자원을 관리할 수 있습니다. 서브 인터프리터 vs 멀티 프로세싱멀티 프로세싱과 비교하면 둘 다 Python CPU Bound 작업에서 코드를 병렬로 실행할 수 있게 해줍니다. 하지만 서브 인터프리터는 동일한 프로세스에서 실행되므로 더 가볍고 커뮤니케이션이 더 빠르고, 멀티 프로세스는 개별 프로세스를 통한 메모리 공간을 사용하기에 더 강한 격리 수준을 제공한다는 차이가 있습니다. 처리 속도 초당 처리 가능 요청 수 위의 표는 서브 인터프리터의 성능 지표입니다. 더 자세한 내용은 다음 영상을 참고해주세요. ","date":"2024-07-11","objectID":"/posts/python-gil/:4:1","series":null,"tags":["GIL","python","multithreading"],"title":"[Python] GIL","uri":"/posts/python-gil/#서브-인터프리터를-통한-병렬-처리"},{"categories":["python"],"content":" 서브 인터프리터를 통한 병렬 처리2023년 10월 2일 출시된 Python 3.12부터 서브 인터프리터 별로 개별적인 GIL을 생성하도록 업데이트 되었습니다. 즉, Python 3.12부터 서브 인터프리터 별로 개별 GIL을 생성하며 CPU Bound 작업에서의 병렬 처리도 가능해졌지만 서브 인터프리터는 아직 C-API만을 제공하고 있고 Python 3.13부터 서브 인터프리터를 표준 라이브러리에 포함하려고 하고 있습니다. Python 서브 인터프리터란 메인 인터프리터와 동일한 프로세스와 동일한 주소 공간을 사용하여 병렬로 Python 코드를 실행하지만 완전히 독립된 자원으로 실행되는 인터프리터입니다. 즉, OS로 부터 할당받은 프로세스의 메모리 공간을 각각의 서브 인터프리터가 독립적으로 사용하는 방식입니다. 서브 인터프리터 vs 멀티 스레딩멀티 스레딩과 비교하면 멀티 스레드는 스레드끼리 동일한 메모리 공간을 사용하기 때문에 자원 공유가 편하지만 공유 자원에 race condition이 발생할 수 있어 동기화에 대한 고려가 필요합니다. 반면 서브 인터프리터는 메모리 공간이 독립적이기에 자원 공유를 위해 별도의 채널을 사용해야하여 자원 공유 속도가 스레드에 비하여 느리지만 원전히 독립적으로 자원을 관리할 수 있습니다. 서브 인터프리터 vs 멀티 프로세싱멀티 프로세싱과 비교하면 둘 다 Python CPU Bound 작업에서 코드를 병렬로 실행할 수 있게 해줍니다. 하지만 서브 인터프리터는 동일한 프로세스에서 실행되므로 더 가볍고 커뮤니케이션이 더 빠르고, 멀티 프로세스는 개별 프로세스를 통한 메모리 공간을 사용하기에 더 강한 격리 수준을 제공한다는 차이가 있습니다. 처리 속도 초당 처리 가능 요청 수 위의 표는 서브 인터프리터의 성능 지표입니다. 더 자세한 내용은 다음 영상을 참고해주세요. ","date":"2024-07-11","objectID":"/posts/python-gil/:4:1","series":null,"tags":["GIL","python","multithreading"],"title":"[Python] GIL","uri":"/posts/python-gil/#서브-인터프리터-vs-멀티-스레딩"},{"categories":["python"],"content":" 서브 인터프리터를 통한 병렬 처리2023년 10월 2일 출시된 Python 3.12부터 서브 인터프리터 별로 개별적인 GIL을 생성하도록 업데이트 되었습니다. 즉, Python 3.12부터 서브 인터프리터 별로 개별 GIL을 생성하며 CPU Bound 작업에서의 병렬 처리도 가능해졌지만 서브 인터프리터는 아직 C-API만을 제공하고 있고 Python 3.13부터 서브 인터프리터를 표준 라이브러리에 포함하려고 하고 있습니다. Python 서브 인터프리터란 메인 인터프리터와 동일한 프로세스와 동일한 주소 공간을 사용하여 병렬로 Python 코드를 실행하지만 완전히 독립된 자원으로 실행되는 인터프리터입니다. 즉, OS로 부터 할당받은 프로세스의 메모리 공간을 각각의 서브 인터프리터가 독립적으로 사용하는 방식입니다. 서브 인터프리터 vs 멀티 스레딩멀티 스레딩과 비교하면 멀티 스레드는 스레드끼리 동일한 메모리 공간을 사용하기 때문에 자원 공유가 편하지만 공유 자원에 race condition이 발생할 수 있어 동기화에 대한 고려가 필요합니다. 반면 서브 인터프리터는 메모리 공간이 독립적이기에 자원 공유를 위해 별도의 채널을 사용해야하여 자원 공유 속도가 스레드에 비하여 느리지만 원전히 독립적으로 자원을 관리할 수 있습니다. 서브 인터프리터 vs 멀티 프로세싱멀티 프로세싱과 비교하면 둘 다 Python CPU Bound 작업에서 코드를 병렬로 실행할 수 있게 해줍니다. 하지만 서브 인터프리터는 동일한 프로세스에서 실행되므로 더 가볍고 커뮤니케이션이 더 빠르고, 멀티 프로세스는 개별 프로세스를 통한 메모리 공간을 사용하기에 더 강한 격리 수준을 제공한다는 차이가 있습니다. 처리 속도 초당 처리 가능 요청 수 위의 표는 서브 인터프리터의 성능 지표입니다. 더 자세한 내용은 다음 영상을 참고해주세요. ","date":"2024-07-11","objectID":"/posts/python-gil/:4:1","series":null,"tags":["GIL","python","multithreading"],"title":"[Python] GIL","uri":"/posts/python-gil/#서브-인터프리터-vs-멀티-프로세싱"},{"categories":["python"],"content":" GIL 비활성화Python 3.13부터는 GIL을 비활성화 하는 옵션을 실험적으로 제공할 예정이라고 합니다. Python 3.13 Docs CPython will run with the global interpreter lock (GIL) disabled when configured using the –disable-gil option at build time. This is an experimental feature and therefore isn’t used by default. Users need to either compile their own interpreter, or install one of the experimental builds that are marked as free-threaded. See PEP 703 “Making the Global Interpreter Lock Optional in CPython” for more detail. ","date":"2024-07-11","objectID":"/posts/python-gil/:4:2","series":null,"tags":["GIL","python","multithreading"],"title":"[Python] GIL","uri":"/posts/python-gil/#gil-비활성화"},{"categories":["python"],"content":"가비지 컬렉션은 프로그래밍 언어에서 더 이상 필요하지 않은 객체를 자동으로 메모리에서 해제하는 메모리 관리 방법입니다. 그렇다면 Python에서는 어떻게 GC를 처리할까요?","date":"2024-07-10","objectID":"/posts/python-gc/","series":null,"tags":["GC","python"],"title":"[Python] Garbage Collection","uri":"/posts/python-gc/"},{"categories":["python"],"content":" Garbage Collection?가비지 컬렉션은 프로그래밍 언어에서 더 이상 필요하지 않은 객체를 자동으로 메모리에서 해제하는 메모리 관리 방법입니다. 그렇다면, 가비지 컬렉션은 왜 필요한 것일까요? ","date":"2024-07-10","objectID":"/posts/python-gc/:1:0","series":null,"tags":["GC","python"],"title":"[Python] Garbage Collection","uri":"/posts/python-gc/#garbage-collection"},{"categories":["python"],"content":" 수동 메모리 관리 방식의 문제점C나 C++ 등의 프로그래밍 언어는 가비지 컬렉션 기능을 포함하지 않고 개발자가 free, malloc의 함수를 통해 수동으로 메모리 관리를 하도록 설계되었습니다. 하지만 수동 메모리 관리 방식은 여러 어려움을 가지고 있습니다. 메모리 누수: 할당한 메모리를 적절히 해제하지 않으면 불필요한 객체들이 남고 점차 더 많은 메모리를 차지하여 프로그램이 종료될 수 있습니다. 이중 해제: 이미 해제된 메모리를 다시 해제하려고 하면 프로그램이 비정상적으로 동작하거나 종료될 수 있습니다. Dangling Pointer: 메모리를 해제한 후에도 해당 메모리를 참조하는 포인터가 남아있는 경우 의도하지 않은 동작이 발생할 수 있습니다. 가비지 컬렉션은 이러한 수동 메모리 관리 문제점을 해결하기 때문에 Python, Java, C#, Go 등의 언어는 자동 가비지 컬렉션을 내장하여 제공합니다. ","date":"2024-07-10","objectID":"/posts/python-gc/:1:1","series":null,"tags":["GC","python"],"title":"[Python] Garbage Collection","uri":"/posts/python-gc/#수동-메모리-관리-방식의-문제점"},{"categories":["python"],"content":" 가비지 컬렉션의 단점하지만 가비지 컬렉션도 몇가지 문제점을 가지고 있습니다. 성능 오버헤드: 가비지 컬렉션을 위해 지속적으로 메모리를 확인하고 정리하는 작업을 수행하는 오버헤드가 발생합니다. 메모리 오버헤드: 가비지 컬렉션을 위한 추가적인 메타데이터를 저장해야하므로 메모리 사용량이 증가합니다. 최적화 어려움: 가비지 컬렉션을 통해 메모리를 관리하므로 개발자가 메모리 할당과 해제를 세밀하게 제어할 수 없습니다. 따라서 최적화가 어려울 수 있습니다. ","date":"2024-07-10","objectID":"/posts/python-gc/:1:2","series":null,"tags":["GC","python"],"title":"[Python] Garbage Collection","uri":"/posts/python-gc/#가비지-컬렉션의-단점"},{"categories":["python"],"content":" Python 가비지 컬렉션","date":"2024-07-10","objectID":"/posts/python-gc/:2:0","series":null,"tags":["GC","python"],"title":"[Python] Garbage Collection","uri":"/posts/python-gc/#python-가비지-컬렉션"},{"categories":["python"],"content":" Reference CountingPython에서 가장 기본적으로 사용하는 가비지 컬렉션 알고리즘입니다. 각 객체는 참조된 횟수인 reference count 변수를 가집니다. 객체는 참조될 때마다 reference count 값을 증가시키고 참조가 해재될 때마다 reference count를 감소시킵니다. Note sys.getrefcount를 통해 해당 객체가 얼마나 참조되고 있는지 확인할 수 있습니다. 객체는 sys.getrefcount에 의해 참조되고 있으므로 참조 횟수는 +1만큼 추가되어 출력됩니다. python \u003e\u003e\u003e x = object() \u003e\u003e\u003e sys.getrefcount(x) 2 \u003e\u003e\u003e y = x \u003e\u003e\u003e sys.getrefcount(x) 3 \u003e\u003e\u003e del y \u003e\u003e\u003e sys.getrefcount(x) 2 reference count 값이 0이 되면 해당 객체는 더 이상 사용되지 않는 것으로 간주되어 메모리에서 해제됩니다. Python은 Reference Counting 방법을 통해 객체가 더 이상 필요하지 않으면 즉시 해제하는 간단하고 즉각적인 메모리 해제를 제공합니다. 하지만, 단순한 Reference Counting 방법은 순환 참조(Circular Reference) 문제를 해결하지 못합니다. ","date":"2024-07-10","objectID":"/posts/python-gc/:2:1","series":null,"tags":["GC","python"],"title":"[Python] Garbage Collection","uri":"/posts/python-gc/#reference-counting"},{"categories":["python"],"content":" 순환 참조 문제순환 참조란 두 개 이상의 객체가 서를 참조하여 참조가 순환을 이루는 상황을 말합니다. 순환 참조가 발생하면 Reference Count로는 객체가 더 이상 사용되지 않는다는 것을 인지할 수 없습니다. ex1.py class A: def __init__(self): self.b = None print(\"A 객체 생성됨\") def __del__(self): print(\"A 객체 삭제됨\") class B: def __init__(self): self.a = None print(\"B 객체 생성됨\") def __del__(self): print(\"B 객체 삭제됨\") if __name__ == '__main__': a = A() b = B() b.a = a del a del b print('프로그램 종료') 위의 ex1.py의 경우 b 객체는 a를 참조하지만 a는 b를 참조하지 않아 순환 참조가 아닌 상황입니다. 따라서 b의 reference count는 1이고 a의 reference count는 2가 됩니다. del a가 실행되면 a 객체를 가리키는 변수는 삭제되지만 a 객체는 b에서 가리키고 있기 때문에 reference count 값이 1이 되고 삭제되지 않습니다. del b가 실행되면 b 객체의 reference count가 0이 되며 삭제되며 a도 reference count가 0이 되며 삭제됩니다. bash $ python ex1.py A 객체 생성됨 B 객체 생성됨 B 객체 삭제됨 A 객체 삭제됨 프로그램 종료 del a를 del b보다 먼저 실행했지만 b 객체가 먼저 삭제되는 것고 이후 프로그램이 종료되는 것을 확인할 수 있습니다. ex2.py class A: def __init__(self): self.b = None print(\"A 객체 생성됨\") def __del__(self): print(\"A 객체 삭제됨\") class B: def __init__(self): self.a = None print(\"B 객체 생성됨\") def __del__(self): print(\"B 객체 삭제됨\") if __name__ == '__main__': a = A() b = B() a.b = b b.a = a del a del b print('프로그램 종료') 위의 ex2.py의 경우 b 객체는 a를 참조하고 a는 b를 참조하여 순환 참조인 상황입니다. 따라서 b와 a의 reference count는 2가 됩니다. del a가 실행되면 a 객체를 가리키는 변수는 삭제되지만 a 객체는 b에서 가리키고 있기 때문에 reference count 값이 1이 되고 삭제되지 않습니다. del b가 실행되면 b 객체를 가리키는 변수는 삭제되지만 b 객체는 a에서 가리키고 있기 때문에 reference count 값이 1이 되고 삭제되지 않습니다. bash $ python ex2.py A 객체 생성됨 B 객체 생성됨 프로그램 종료 A 객체 삭제됨 B 객체 삭제됨 따라서 del 명령어로는 a와 b 객체가 삭제되지 않고, 프로그램이 종료된 이후 객체가 삭제됩니다. 이를 통해 순환 참조 객체는 reference count를 통해 삭제할 수 없다는 것을 확인할 수 있습니다. Tip 위의 예제는 Python 코드 시각화 툴인 Python Tutor에서 확인하면 더 쉽게 이해할 수 있습니다. ","date":"2024-07-10","objectID":"/posts/python-gc/:2:2","series":null,"tags":["GC","python"],"title":"[Python] Garbage Collection","uri":"/posts/python-gc/#순환-참조-문제"},{"categories":["python"],"content":" 순환 참조 해결Python의 가비지 컬렉터인 gc 모듈은 주기적으로 힙 메모리를 검사하여 순환 참조로 인해 reference count가 0이 되지 않는 객체를 찾아내고 해제합니다. 순환 참조 판별그렇다면, gc 모듈은 어떻게 순환 참조로 인해 reference count가 0이 되지 않는 접근 불가능한 객체를 판별할까요? 우선, 순환 참조는 컨테이너 객체(array, dictionary, list, class)에서만 발생할 수 있습니다. 따라서 gc 모듈은 컨테이너 객체에만 집중하게 됩니다. 다음은 순환 참조중인 객체를 삭제하는 예제입니다. python \u003e\u003e\u003e import gc \u003e\u003e\u003e class Link: ... def __init__(self, next_link=None): ... self.next_link = next_link \u003e\u003e\u003e link_3 = Link() \u003e\u003e\u003e link_2 = Link(link_3) \u003e\u003e\u003e link_1 = Link(link_2) \u003e\u003e\u003e link_3.next_link = link_1 \u003e\u003e\u003e A = link_1 # link_1, link_2, link_3가 삭제되더라도 A에서 여전히 참조중이므로 삭제되지 않음 \u003e\u003e\u003e del link_1, link_2, link_3 \u003e\u003e\u003e link_4 = Link() \u003e\u003e\u003e link_4.next_link = link_4 \u003e\u003e\u003e del link_4 # 순환 참조중인 접근 불가능한 link_4 객체 수집 (객체의 __dict__ 객체도 함께 수집) \u003e\u003e\u003e gc.collect() 2 link_1, link_2, link_3는 서로 순환 참조하고 삭제되었지만 A가 link_1을 참조하고 있어 접근 가능하고 삭제되면 안되는 경우입니다. 반면 link_4는 스스로를 참조하는 순환 참조 상황이지만 삭제되어 해제되어야 하는 경우입니다. 우선 gc는 모든 컨테이너 객체를 스캔할 객체 리스트(Objects to Scan)에 저장합니다. 저장된 모든 객체는 reference count 값을 복사하여 gc_ref라는 접근 불가능 여부 판별을 위한 추가 필드를 가지게 됩니다. 이후 gc는 스캔할 객체 리스트를 순환하며 한 객체가 참조하는 다른 객체들의 gc_ref값을 1만큼 줄입니다. 즉, 우선 link_1을 확인하고 link_1에서 참조하는 link_2의 gc_ref 값을 0으로 줄이고 이후 link_2를 확인하고 link_2에서 참조하는 link_3의 gc_ref 값을 0으로 줄이는 방식입니다. 여기서, gc_ref 값이 0인 객체들은 컨테이너 객체 외부에서 현재 참조되고 있지 않기에 접근 불가능 할수도 있는 객체들입니다. 하지만 link_2와 같이 외부에서 연결된 link_1과 연결될 수 있기에 무조건 접근 불가능한 것은 아닙니다. 반면 gc_ref 값이 0보다 크다면 모두 접근 가능한 객체입니다. 우선 gc는 스캔할 객체 리스트를 순환하며 gc_ref값이 0인 객체들을 접근 불가능할 수도 있는 리스트(Unreachable ?)에 옮깁니다. 다음은 link_1과 link_2는 아직 순환하지 않고 link_3, link_4만 순환한 상태입니다. 여기서 gc가 link_1 객체를 순환하면 link_1의 gc_ref가 0보다 크기 때문에 접근 가능한 객체로 판별하고 link_1에서 접근 가능한 모든 객체를 찾아 gc_ref를 1로 설정하여 접근 가능하도록 표시하게 됩니다. 또한, 한번 순환한 객체를 다시 순환하지 않기 위해 방문 여부를 체크해둡니다. 모든 순환이 완료되면 gc는 접근 가능한 객체와 접근 불가능한 객체를 구분할 수 있습니다. 접근 불가능한 객체 삭제gc가 모든 접근 불가능한 객체를 판별하면, 매우 작은 프로세스를 시작하여 객체가 종료될 시 발생하는 모든 finalizer를 호출하고 reference count를 0으로 줄여 해당 객체를 삭제합니다. 이를 통해 Python은 순환 객체를 판별하고 삭제할 수 있습니다. ","date":"2024-07-10","objectID":"/posts/python-gc/:2:3","series":null,"tags":["GC","python"],"title":"[Python] Garbage Collection","uri":"/posts/python-gc/#순환-참조-해결"},{"categories":["python"],"content":" 순환 참조 해결Python의 가비지 컬렉터인 gc 모듈은 주기적으로 힙 메모리를 검사하여 순환 참조로 인해 reference count가 0이 되지 않는 객체를 찾아내고 해제합니다. 순환 참조 판별그렇다면, gc 모듈은 어떻게 순환 참조로 인해 reference count가 0이 되지 않는 접근 불가능한 객체를 판별할까요? 우선, 순환 참조는 컨테이너 객체(array, dictionary, list, class)에서만 발생할 수 있습니다. 따라서 gc 모듈은 컨테이너 객체에만 집중하게 됩니다. 다음은 순환 참조중인 객체를 삭제하는 예제입니다. python \u003e\u003e\u003e import gc \u003e\u003e\u003e class Link: ... def __init__(self, next_link=None): ... self.next_link = next_link \u003e\u003e\u003e link_3 = Link() \u003e\u003e\u003e link_2 = Link(link_3) \u003e\u003e\u003e link_1 = Link(link_2) \u003e\u003e\u003e link_3.next_link = link_1 \u003e\u003e\u003e A = link_1 # link_1, link_2, link_3가 삭제되더라도 A에서 여전히 참조중이므로 삭제되지 않음 \u003e\u003e\u003e del link_1, link_2, link_3 \u003e\u003e\u003e link_4 = Link() \u003e\u003e\u003e link_4.next_link = link_4 \u003e\u003e\u003e del link_4 # 순환 참조중인 접근 불가능한 link_4 객체 수집 (객체의 __dict__ 객체도 함께 수집) \u003e\u003e\u003e gc.collect() 2 link_1, link_2, link_3는 서로 순환 참조하고 삭제되었지만 A가 link_1을 참조하고 있어 접근 가능하고 삭제되면 안되는 경우입니다. 반면 link_4는 스스로를 참조하는 순환 참조 상황이지만 삭제되어 해제되어야 하는 경우입니다. 우선 gc는 모든 컨테이너 객체를 스캔할 객체 리스트(Objects to Scan)에 저장합니다. 저장된 모든 객체는 reference count 값을 복사하여 gc_ref라는 접근 불가능 여부 판별을 위한 추가 필드를 가지게 됩니다. 이후 gc는 스캔할 객체 리스트를 순환하며 한 객체가 참조하는 다른 객체들의 gc_ref값을 1만큼 줄입니다. 즉, 우선 link_1을 확인하고 link_1에서 참조하는 link_2의 gc_ref 값을 0으로 줄이고 이후 link_2를 확인하고 link_2에서 참조하는 link_3의 gc_ref 값을 0으로 줄이는 방식입니다. 여기서, gc_ref 값이 0인 객체들은 컨테이너 객체 외부에서 현재 참조되고 있지 않기에 접근 불가능 할수도 있는 객체들입니다. 하지만 link_2와 같이 외부에서 연결된 link_1과 연결될 수 있기에 무조건 접근 불가능한 것은 아닙니다. 반면 gc_ref 값이 0보다 크다면 모두 접근 가능한 객체입니다. 우선 gc는 스캔할 객체 리스트를 순환하며 gc_ref값이 0인 객체들을 접근 불가능할 수도 있는 리스트(Unreachable ?)에 옮깁니다. 다음은 link_1과 link_2는 아직 순환하지 않고 link_3, link_4만 순환한 상태입니다. 여기서 gc가 link_1 객체를 순환하면 link_1의 gc_ref가 0보다 크기 때문에 접근 가능한 객체로 판별하고 link_1에서 접근 가능한 모든 객체를 찾아 gc_ref를 1로 설정하여 접근 가능하도록 표시하게 됩니다. 또한, 한번 순환한 객체를 다시 순환하지 않기 위해 방문 여부를 체크해둡니다. 모든 순환이 완료되면 gc는 접근 가능한 객체와 접근 불가능한 객체를 구분할 수 있습니다. 접근 불가능한 객체 삭제gc가 모든 접근 불가능한 객체를 판별하면, 매우 작은 프로세스를 시작하여 객체가 종료될 시 발생하는 모든 finalizer를 호출하고 reference count를 0으로 줄여 해당 객체를 삭제합니다. 이를 통해 Python은 순환 객체를 판별하고 삭제할 수 있습니다. ","date":"2024-07-10","objectID":"/posts/python-gc/:2:3","series":null,"tags":["GC","python"],"title":"[Python] Garbage Collection","uri":"/posts/python-gc/#순환-참조-판별"},{"categories":["python"],"content":" 순환 참조 해결Python의 가비지 컬렉터인 gc 모듈은 주기적으로 힙 메모리를 검사하여 순환 참조로 인해 reference count가 0이 되지 않는 객체를 찾아내고 해제합니다. 순환 참조 판별그렇다면, gc 모듈은 어떻게 순환 참조로 인해 reference count가 0이 되지 않는 접근 불가능한 객체를 판별할까요? 우선, 순환 참조는 컨테이너 객체(array, dictionary, list, class)에서만 발생할 수 있습니다. 따라서 gc 모듈은 컨테이너 객체에만 집중하게 됩니다. 다음은 순환 참조중인 객체를 삭제하는 예제입니다. python \u003e\u003e\u003e import gc \u003e\u003e\u003e class Link: ... def __init__(self, next_link=None): ... self.next_link = next_link \u003e\u003e\u003e link_3 = Link() \u003e\u003e\u003e link_2 = Link(link_3) \u003e\u003e\u003e link_1 = Link(link_2) \u003e\u003e\u003e link_3.next_link = link_1 \u003e\u003e\u003e A = link_1 # link_1, link_2, link_3가 삭제되더라도 A에서 여전히 참조중이므로 삭제되지 않음 \u003e\u003e\u003e del link_1, link_2, link_3 \u003e\u003e\u003e link_4 = Link() \u003e\u003e\u003e link_4.next_link = link_4 \u003e\u003e\u003e del link_4 # 순환 참조중인 접근 불가능한 link_4 객체 수집 (객체의 __dict__ 객체도 함께 수집) \u003e\u003e\u003e gc.collect() 2 link_1, link_2, link_3는 서로 순환 참조하고 삭제되었지만 A가 link_1을 참조하고 있어 접근 가능하고 삭제되면 안되는 경우입니다. 반면 link_4는 스스로를 참조하는 순환 참조 상황이지만 삭제되어 해제되어야 하는 경우입니다. 우선 gc는 모든 컨테이너 객체를 스캔할 객체 리스트(Objects to Scan)에 저장합니다. 저장된 모든 객체는 reference count 값을 복사하여 gc_ref라는 접근 불가능 여부 판별을 위한 추가 필드를 가지게 됩니다. 이후 gc는 스캔할 객체 리스트를 순환하며 한 객체가 참조하는 다른 객체들의 gc_ref값을 1만큼 줄입니다. 즉, 우선 link_1을 확인하고 link_1에서 참조하는 link_2의 gc_ref 값을 0으로 줄이고 이후 link_2를 확인하고 link_2에서 참조하는 link_3의 gc_ref 값을 0으로 줄이는 방식입니다. 여기서, gc_ref 값이 0인 객체들은 컨테이너 객체 외부에서 현재 참조되고 있지 않기에 접근 불가능 할수도 있는 객체들입니다. 하지만 link_2와 같이 외부에서 연결된 link_1과 연결될 수 있기에 무조건 접근 불가능한 것은 아닙니다. 반면 gc_ref 값이 0보다 크다면 모두 접근 가능한 객체입니다. 우선 gc는 스캔할 객체 리스트를 순환하며 gc_ref값이 0인 객체들을 접근 불가능할 수도 있는 리스트(Unreachable ?)에 옮깁니다. 다음은 link_1과 link_2는 아직 순환하지 않고 link_3, link_4만 순환한 상태입니다. 여기서 gc가 link_1 객체를 순환하면 link_1의 gc_ref가 0보다 크기 때문에 접근 가능한 객체로 판별하고 link_1에서 접근 가능한 모든 객체를 찾아 gc_ref를 1로 설정하여 접근 가능하도록 표시하게 됩니다. 또한, 한번 순환한 객체를 다시 순환하지 않기 위해 방문 여부를 체크해둡니다. 모든 순환이 완료되면 gc는 접근 가능한 객체와 접근 불가능한 객체를 구분할 수 있습니다. 접근 불가능한 객체 삭제gc가 모든 접근 불가능한 객체를 판별하면, 매우 작은 프로세스를 시작하여 객체가 종료될 시 발생하는 모든 finalizer를 호출하고 reference count를 0으로 줄여 해당 객체를 삭제합니다. 이를 통해 Python은 순환 객체를 판별하고 삭제할 수 있습니다. ","date":"2024-07-10","objectID":"/posts/python-gc/:2:3","series":null,"tags":["GC","python"],"title":"[Python] Garbage Collection","uri":"/posts/python-gc/#접근-불가능한-객체-삭제"},{"categories":["python"],"content":" 최적화 (Generation)그렇다면 순환 참조를 해결하기 위한 가비지 컬렉션은 언제 동작할까요? Python의 가비지 컬렉션은 모든 컨테이너 객체를 순환해야하기 때문에 매번 동작하게 되면 큰 오버헤드가 발생합니다. gc는 이를 위해 유명한 최적화 방법인 generations 기법을 사용합니다. generations의 가장 핵심 아이디어는 대부분의 객체는 매우 짧은 생명 주기를 가지고있고 생성된 이후 곧 수집된다는 것입니다. 이를 통해 모든 컨테이너 객체는 3가지 세대로 구분되게 됩니다. 0세대: 바로 생성된 객체 1세대: 0세대 중 접근 불가능한 객체 판별 알고리즘에서 수집되지 않은 객체 (수집 빈도 적음) 2세대: 1세대 중 접근 불가능한 객체 판별 알고리즘에서 수집되지 않은 객체 (수집 빈도 매우 적음) gc는 0세대일수록 더 빠르게 수집된다는 generational hypothesis에 근거하여 0세대일수록 더 자주 가비지 컬렉션을 수행하도록 설계되었습니다. gc는 각 세대에 threshold값을 설정하고 특정 세대에 threshold값 만큼의 객체가 존재한다면 가비지 컬렉션을 수행합니다. python \u003e\u003e\u003e import gc \u003e\u003e\u003e gc.get_threshold() (700, 10, 10) 위는 gc의 기본 threshold 값이고 0세대에 700, 1세대에 10, 2세대에 10으로 설정되어 있는 것을 확인할 수 있습니다. 여기서 0세대의 threshold는 할당되었지만 삭제되지 않은 객체의 수를 의미하고 이후부터 1세대는 0세대가 가비지 컬렉션을 수행하고 접근 불가능한 객체가 있다면 +1을 하고 만약 1세대가 10만큼 되어 가비지 컬렉션을 수행하고 접근 불가능한 객체가 있다면 2세대에 +1을 하는 방식입니다. python \u003e\u003e\u003e import gc \u003e\u003e\u003e class MyObj: ... pass ... \u003e\u003e\u003e gc.collect() 0 \u003e\u003e\u003e gc.get_count() (21, 0, 0) \u003e\u003e\u003e x = MyObj() \u003e\u003e\u003e x.self = x \u003e\u003e\u003e y = MyObj() \u003e\u003e\u003e y.self = y \u003e\u003e\u003e gc.get_objects(generation=0) [\u003c__main__.MyObj object at 0x105430b90\u003e, \u003c__main__.MyObj object at 0x105430f90\u003e, ...] \u003e\u003e\u003e gc.get_count() (27, 0, 0) \u003e\u003e\u003e gc.collect(generation=0) 0 \u003e\u003e\u003e gc.get_objects(generation=0) [] \u003e\u003e\u003e gc.get_objects(generation=1) [..., \u003c__main__.MyObj object at 0x104e41210\u003e, \u003c__main__.MyObj object at 0x104e41550\u003e, ...] \u003e\u003e\u003e gc.get_count() (7, 1, 0) 위와 같이 gc.get_objects(generation=NUM)을 통해 객체 리스트를 확인할 수도 있고 gc.collect(generation=NUM)을 통해 직접 가비지 컬렉션을 할 수도 있습니다. ","date":"2024-07-10","objectID":"/posts/python-gc/:2:4","series":null,"tags":["GC","python"],"title":"[Python] Garbage Collection","uri":"/posts/python-gc/#최적화-generation"},{"categories":["python"],"content":"Black은 Python의 엄격한 코드 스타일 자동화 도구입니다.","date":"2024-07-07","objectID":"/posts/black/","series":null,"tags":["linter","black"],"title":"[Python] Black을 통한 코드 스타일 자동화","uri":"/posts/black/"},{"categories":["python"],"content":" 코드 스타일이란?코드 스타일은 소스 코드 작성 시 따르는 일련의 규칙과 가이드라인을 말합니다. 개인 프로젝트에서는 개인의 선호에 따라 코드 스타일을 정하여도 문제가 없지만 공동 작업을 하는 협업 프로젝트에서는 개발자간 상이한 코드 스타일이 코드를 빠르게 파악하는데 어려움을 줍니다. 따라서 팀에서는 여러 개발자들이 통일된 코딩 스타일을 갖도록 규약을 정하는데 이것을 코딩 컨벤션(Coding Convention)이라고 합니다. python의 경우 PEP8을 통해 코드 스타일 규약을 정해두었고 구글과 같은 팀에서도 자신만의 구글 스타일 가이드를 가지고 있습니다. ","date":"2024-07-07","objectID":"/posts/black/:1:0","series":null,"tags":["linter","black"],"title":"[Python] Black을 통한 코드 스타일 자동화","uri":"/posts/black/#코드-스타일이란"},{"categories":["python"],"content":" Black이란?Black은 Python의 엄격한 코드 스타일 자동화 도구입니다. Black은 PEP8에 기반한 엄격한 코드 스타일 자동화를 제공하며 사용자들이 코드 스타일에 대해 고민하고 논의하지 않고 개발에만 집중할 수 있도록 해줍니다. 즉, 블랙은 모든 블랙으로 포메팅된 프로젝트를 동일한 구조를 가지도록 하여 개발자들이 내용에만 집중할 수 있도록 해줍니다. Black 플레이그라운드에서 Black에서 제공하는 포메팅을 체험해볼 수 있습니다. ","date":"2024-07-07","objectID":"/posts/black/:2:0","series":null,"tags":["linter","black"],"title":"[Python] Black을 통한 코드 스타일 자동화","uri":"/posts/black/#black이란"},{"categories":["python"],"content":" Black 설치 방법pip를 통하여 손쉽게 Black을 설치할 수 있습니다. bash pip install black ","date":"2024-07-07","objectID":"/posts/black/:3:0","series":null,"tags":["linter","black"],"title":"[Python] Black을 통한 코드 스타일 자동화","uri":"/posts/black/#black-설치-방법"},{"categories":["python"],"content":" Black 사용 방법 example.py x = {'a': 30, 'b': 20, 'c': 10} def func_a(): print('func a') def func_b(): print('func b') if __name__ == '__main__': func_a() func_b() 위와 같이 indent가 뒤죽박죽으로 작성된 가독성이 떨어지는 파일을 블랙으로 포메팅해보도록 하겠습니다. bash black example.py \u003e reformatted example.py All done! ✨ 🍰 ✨ 1 file reformatted. example.py x = {\"a\": 30, \"b\": 20, \"c\": 10} def func_a(): print(\"func a\") def func_b(): print(\"func b\") if __name__ == \"__main__\": func_a() func_b() Black을 통해 일정한 방식으로 포메팅 된 것을 확인할 수 있습니다. black {DIRECTORY}를 통해 특정 디렉토리내에 모든 파일에 대해 Black 포메팅을 수행할 수도 있습니다. ","date":"2024-07-07","objectID":"/posts/black/:4:0","series":null,"tags":["linter","black"],"title":"[Python] Black을 통한 코드 스타일 자동화","uri":"/posts/black/#black-사용-방법"},{"categories":["python"],"content":" Black 커스텀 설정black 명령어의 옵션을 통해 커스텀한 설정을 할 수도 있고 toml 파일을 통해 config를 설정할 수도 있습니다. ","date":"2024-07-07","objectID":"/posts/black/:5:0","series":null,"tags":["linter","black"],"title":"[Python] Black을 통한 코드 스타일 자동화","uri":"/posts/black/#black-커스텀-설정"},{"categories":["python"],"content":" 옵션을 통한 설정 example.py x = {'a': 30, 'b': 20, 'c': 10} def func_a(): print('func a') def func_b(): print('func b') if __name__ == '__main__': func_a() func_b() bash black example.py --skip-string-normalization 위의 example.py 파일을 다시 포메팅해보면 이번에는 '을 \"로 변환하는 작업을 수행하지 않는 것을 볼 수 있습니다. example.py x = {'a': 30, 'b': 20, 'c': 10} def func_a(): print('func a') def func_b(): print('func b') if __name__ == '__main__': func_a() func_b() ","date":"2024-07-07","objectID":"/posts/black/:5:1","series":null,"tags":["linter","black"],"title":"[Python] Black을 통한 코드 스타일 자동화","uri":"/posts/black/#옵션을-통한-설정"},{"categories":["python"],"content":" toml 파일 설정pyproject.toml 파일은 python의 패키징을 위한 설정 파일입니다. pyproject.toml 파일의 [tool.black] 테이블을 통해 Black 설정이 가능합니다. pyproject.toml [tool.black] skip-string-normalization = true 위와 같이 pyproject.toml 파일을 작성하고 black 명령어를 실행시키면 이전에 black example.py --skip-string-normalization을 실행한 것과 동일하게 동작하는 것을 확인할 수 있습니다. 이외에도 exclude 옵션을 통해 불필요한 파일을 포메팅하지 않거나 line-length를 통해 라인 별 최대 글자 수를 수정할 수도 있습니다. ","date":"2024-07-07","objectID":"/posts/black/:5:2","series":null,"tags":["linter","black"],"title":"[Python] Black을 통한 코드 스타일 자동화","uri":"/posts/black/#toml-파일-설정"},{"categories":["python"],"content":" 자동화 설정매번 Black 명령을 수동으로 실행할 수도 있지만 IDE 설정이나 Git Hook의 pre-commit 설정, 아니면 Github Action 등에 설정하며 자동화할 수도 있습니다. ","date":"2024-07-07","objectID":"/posts/black/:6:0","series":null,"tags":["linter","black"],"title":"[Python] Black을 통한 코드 스타일 자동화","uri":"/posts/black/#자동화-설정"},{"categories":["python"],"content":" Pycharm 설정Preferences or Settings -\u003e Tools -\u003e Black에서 On save 옵션을 체크하여 매 저장마다 자동화된 포메팅을 제공할 수 있습니다. ","date":"2024-07-07","objectID":"/posts/black/:6:1","series":null,"tags":["linter","black"],"title":"[Python] Black을 통한 코드 스타일 자동화","uri":"/posts/black/#pycharm-설정"},{"categories":["python"],"content":" Git Hook Pre-Commit 설정pre-commit은 git commit을 하기 전에 자동으로 설정해둔 작업들이 실행되는 툴입니다. .git/hooks/pre-commit 파일을 통하여 작업이 진행되므로 직접 해당 파일을 생성할 수도 있지만 python에서 pre-commit 설정을 돕는 라이브러리를 통해 설정하도록 하겠습니다. bash pip install pre-commit 다음과 같이 .pre-commit-config.yaml 파일을 생성해줍니다. .pre-commit-config.yaml repos: - repo: https://github.com/psf/black-pre-commit-mirror rev: 24.4.2 hooks: - id: black 다음 명령어를 통해 .git/hooks/pre-commit을 생성합니다. bash pre-commit install 그러면 다음과 같이 pre-commit 파일이 생성되어있는 것을 확인할 수 있습니다. bash cat .git/hooks/pre-commit example.py x = {'a': 30, 'b': 20, 'c': 10} def func_a(): print('func a') def func_b(): print('func b') if __name__ == '__main__': func_a() func_b() 기존 포메팅되지 않은 example.py 파일을 커밋해보도록 하겠습니다. bash git add example.py git commit -m 'Add: example.py' \u003e black....................................................................Failed - hook id: black - files were modified by this hook reformatted example.py All done! ✨ 🍰 ✨ 1 file reformatted. 그러자 위와 같이 black에서 실패했다는 로그가 출력되고 example.py가 포메팅되어있는 것을 확인할 수 있습니다. bash git add example.py git commit -m 'Add: example.py' 다시 commit을 하자 포메팅된 파일이 잘 등록되는 것을 확인할 수 있습니다. ","date":"2024-07-07","objectID":"/posts/black/:6:2","series":null,"tags":["linter","black"],"title":"[Python] Black을 통한 코드 스타일 자동화","uri":"/posts/black/#git-hook-pre-commit-설정"},{"categories":["python"],"content":" Github Action 등록.github/workflows/black.yml 파일을 깃허브 레포지토리에 생성하여 깃허브 액션 워크플로우를 수행할 수 있습니다. black.yml name: Black on: [push, pull_request] jobs: black: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: psf/black@stable 그러면 매번 Push와 PR에서 Black을 체크할 수 있습니다. ","date":"2024-07-07","objectID":"/posts/black/:6:3","series":null,"tags":["linter","black"],"title":"[Python] Black을 통한 코드 스타일 자동화","uri":"/posts/black/#github-action-등록"},{"categories":["python"],"content":"Poetry는 Python의 의존성 관리 및 패키징 도구입니다.","date":"2024-07-05","objectID":"/posts/poetry/","series":null,"tags":["poetry","package","VirtualEnv"],"title":"[Python] Poetry를 통한 Python 개발 환경 구축","uri":"/posts/poetry/"},{"categories":["python"],"content":" Poetry란?Poetry는 Python의 의존성 관리 및 패키징 도구입니다. ","date":"2024-07-05","objectID":"/posts/poetry/:1:0","series":null,"tags":["poetry","package","VirtualEnv"],"title":"[Python] Poetry를 통한 Python 개발 환경 구축","uri":"/posts/poetry/#poetry란"},{"categories":["python"],"content":" 주요 기능 requirements.txt, setup.py, setup.cfg, MANIFEST.in 등의 파일들을 대체하여 pyproject.toml 하나로 모든 설정을 관리합니다. npm의 package-lock.json과 비슷한 poetry.lock 파일을 제공하여 각 패키지의 해시값과 의존성 트리를 기록하여 의존성 충돌을 방지하고 일관된 환경을 유지할 수 있습니다. 손쉽게 개별 가상 환경을 구축하여 프로젝트 관리를 용이하게 합니다. 편리한 빌드 및 배포 기능을 제공합니다. ","date":"2024-07-05","objectID":"/posts/poetry/:2:0","series":null,"tags":["poetry","package","VirtualEnv"],"title":"[Python] Poetry를 통한 Python 개발 환경 구축","uri":"/posts/poetry/#주요-기능"},{"categories":["python"],"content":" 기존 패키지 관리 방법Python의 기본 패키지 관리 도구는 pip를 사용하며, requirements.txt 파일을 통해 패키지 정보를 관리합니다. requirements.txt Django==4.1.13 djangorestframework==3.15.1 bash pip install -r requirements.txt Info requirements.txt를 직접 작성하지 않고 pip로 설치한 후에 pip freeze \u003e requirements.txt와 같이 생성할 수도 있지만 의존 패키지까지 모두 포함시켜 가독성이 떨어지고 관리가 어려워진다는 단점이 있습니다. ","date":"2024-07-05","objectID":"/posts/poetry/:3:0","series":null,"tags":["poetry","package","VirtualEnv"],"title":"[Python] Poetry를 통한 Python 개발 환경 구축","uri":"/posts/poetry/#기존-패키지-관리-방법"},{"categories":["python"],"content":" 기존 가상 환경 구축 방법python에서는 기본적으로 venv 모듈을 통하여 가상 환경 기능을 제공합니다. bash python -m venv venv 해당 명령어로 가상 환경 디렉토리가 생성되면 가상 환경을 활성화 할 수 있습니다. bash source venv/bin/activate deactivate를 명령어를 통해 가상 환경을 종료할 수 있습니다. ","date":"2024-07-05","objectID":"/posts/poetry/:4:0","series":null,"tags":["poetry","package","VirtualEnv"],"title":"[Python] Poetry를 통한 Python 개발 환경 구축","uri":"/posts/poetry/#기존-가상-환경-구축-방법"},{"categories":["python"],"content":" Poetry의 패키지 관리 및 가상환경 구축 방법poetry를 사용하기에 앞서 poetry를 설치하도록 하겠습니다. bash pip install poetry ","date":"2024-07-05","objectID":"/posts/poetry/:5:0","series":null,"tags":["poetry","package","VirtualEnv"],"title":"[Python] Poetry를 통한 Python 개발 환경 구축","uri":"/posts/poetry/#poetry의-패키지-관리-및-가상환경-구축-방법"},{"categories":["python"],"content":" Poetry 설정poetry로 프로젝트를 생성하기 위해서는 poetry new 명령어를, 기존 프로젝트에 poetry를 추가하기 위해서는 poetry init을 사용하여 기본 설정을 할 수 있습니다. bash poetry new myproject --src poetry로 프로젝트를 생성부터 진행하도록 하겠습니다. 여기서 --src 옵션은 src 하위에 패키지가 위치하도록 설정하는 옵션입니다. 그러면 아래와 같은 구조의 프로젝트가 생성됩니다. text ├── README.md ├── pyproject.toml ├── src │ └── myproject │ └── __init__.py └── tests └── __init__.py ","date":"2024-07-05","objectID":"/posts/poetry/:5:1","series":null,"tags":["poetry","package","VirtualEnv"],"title":"[Python] Poetry를 통한 Python 개발 환경 구축","uri":"/posts/poetry/#poetry-설정"},{"categories":["python"],"content":" Poetry 패키지 관리다음과 같이 원하는 패키지를 손쉽게 설치하고 제거할 수 있습니다. bash poetry add {PACKAGE} poetry remove {PACKAGE} 패키지 설치 시에는 버전을 명시하는 형태로 원하는 버전을 설정할 수도 있습니다. bash poetry add django=4.2.3 Requirement Versions Allowed ^1.2 \u003e=1.2.0 \u003c 2.0.0 ~1.2 \u003e=1.2.0 \u003c 1.3.0 1.2.* \u003e=1.2.0 \u003c 1.3.0 혹은 위와 같은 표현식으로 버전을 명시할 수도 있습니다. bash poetry add django@~4.2 이후 install 명령을 통해 프로젝트의 pyproject.toml 파일을 읽고 의존성을 해결하고 새로운 가상환경에 필요한 패키지를 설치합니다. 그러면 poetry.lock 파일이 작성되고 모든 패키지의 해시값과 의존성이 작성된 것을 확인할 수 있습니다. bash poetry install ","date":"2024-07-05","objectID":"/posts/poetry/:5:2","series":null,"tags":["poetry","package","VirtualEnv"],"title":"[Python] Poetry를 통한 Python 개발 환경 구축","uri":"/posts/poetry/#poetry-패키지-관리"},{"categories":["python"],"content":" Poetry 가상환경 관리install 명령을 통해 생성된 가상환경은 env info 명령을 통해 확인할 수 있습니다. bash poetry env info \u003e Virtualenv Python: 3.12.4 Implementation: CPython Path: /Users/joonshik/Library/Caches/pypoetry/virtualenvs/myproject-6Uz1uM9E-py3.12 Executable: /Users/joonshik/Library/Caches/pypoetry/virtualenvs/myproject-6Uz1uM9E-py3.12/bin/python Valid: True 이후 poetry run 명령어나 poetry shell을 통해 Poetry 가상 환경을 통해 명령을 실행시킬 수 있습니다. bash which python poetry run which python which python은 기본 python으로 poetry run which python은 poetry 가상 환경으로 실행되어 출력되는 것을 확인할 수 있습니다. 다음과 같이 python 명령어를 실행시킬 수 있습니다. bash poetry run pip list poetry run python app.py poetry shell 명령어를 통해서 현재 쉘에 가상 환경을 활성화할 수 있습니다. bash poetry shell which python which python을 통해 poetry 가상 환경이 활성화된 것을 확인할 수 있습니다. bash exit exit 명령을 통해 가상 환경을 종료할 수 있습니다. 만약 가상 환경을 사용하지 않고 기본 python 환경을 사용하고 싶다면 poetry config 설정을 할 수도 있습니다. bash poetry config virtualenvs.create false --local 만약 이미 가상환경을 생성했다면 삭제한 후 다시 poetry install을 실행하면 가상 환경이 생성되지 않는 것을 확인할 수 있습니다. bash poetry env list 위의 출력된 가상환경을 remove 명령어를 통해 삭제할 수 있습니다. bash poetry env remove {ENV} 그러면 poetry install을 하더라도 가상환경이 생성되지 않는 것을 확인할 수 있습니다. bash poetry install poetry run which python \u003e Skipping virtualenv creation, as specified in config file. ","date":"2024-07-05","objectID":"/posts/poetry/:5:3","series":null,"tags":["poetry","package","VirtualEnv"],"title":"[Python] Poetry를 통한 Python 개발 환경 구축","uri":"/posts/poetry/#poetry-가상환경-관리"},{"categories":["python"],"content":" Poetry 추가 기능","date":"2024-07-05","objectID":"/posts/poetry/:6:0","series":null,"tags":["poetry","package","VirtualEnv"],"title":"[Python] Poetry를 통한 Python 개발 환경 구축","uri":"/posts/poetry/#poetry-추가-기능"},{"categories":["python"],"content":" 그룹을 통한 의존성 관리Poetry는 그룹을 통한 의존성 관리 기능을 제공합니다. 즉, 개발, 테스트, 운영 등의 환경을 구분하여 패키지를 관리할 수 있도록 해줍니다. 기존의 requirements.txt를 통해 환경을 분리할 때에는 개별 파일을 생성하는 방식으로 처리했습니다. text ├── requirements │ ├── common.txt │ ├── dev.txt │ └── prod.txt dev.txt -r common.txt dev_req==1.0 하지만 Poetry는 복잡한 작업 없이 손쉽게 그룹을 구분할 수 있도록 합니다. bash poetry add black --group dev bash poetry install --without dev poetry install --with dev ","date":"2024-07-05","objectID":"/posts/poetry/:6:1","series":null,"tags":["poetry","package","VirtualEnv"],"title":"[Python] Poetry를 통한 Python 개발 환경 구축","uri":"/posts/poetry/#그룹을-통한-의존성-관리"},{"categories":["python"],"content":" 패키지 통합 관리black이나 pytest 등 여러 라이브러리에서 Poetry를 통한 설정을 지원하여 하나의 파일에서 손쉽게 관리할 수 있습니다. pyproject.toml [tool.pytest.ini_options] minversion = \"6.0\" addopts = \"-ra -q\" testpaths = [ \"tests\", \"integration\", ] ","date":"2024-07-05","objectID":"/posts/poetry/:6:2","series":null,"tags":["poetry","package","VirtualEnv"],"title":"[Python] Poetry를 통한 Python 개발 환경 구축","uri":"/posts/poetry/#패키지-통합-관리"}]